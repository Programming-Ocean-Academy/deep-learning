{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìñ Replication of *‚ÄúEffective Approaches to Attention-based Neural Machine Translation‚Äù*  \n",
        "*(Luong, Pham, Manning, 2015, Stanford NLP Group)*\n",
        "# https://arxiv.org/pdf/1508.04025\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Abstract\n",
        "This paper explores **architectures for attention-based NMT**, introducing two main variants:  \n",
        "- **Global attention**: attends to *all source words* at every decoding step.  \n",
        "- **Local attention**: attends to a *subset (window)* of source words, making it more efficient.  \n",
        "\n",
        "Additionally, the paper proposes the **input-feeding approach**, where past alignment decisions are fed into the decoder to improve consistency.  \n",
        "On large-scale English‚ÄìGerman WMT tasks, these models achieve **significant BLEU improvements** (up to +5.0 BLEU) over strong baselines, setting new state-of-the-art results.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Purpose\n",
        "- Investigate **different types of attention mechanisms** for neural machine translation (NMT).  \n",
        "- Compare **global vs. local attention** for accuracy and efficiency.  \n",
        "- Explore **alignment functions** (dot, general, concat, location).  \n",
        "- Improve the **handling of long sentences** and rare words in translation.  \n",
        "- Establish stronger NMT baselines beyond Bahdanau et al. (2014).\n",
        "\n",
        "---\n",
        "\n",
        "## üî¨ Methodology\n",
        "- **Model backbone**: 4-layer stacked LSTMs (encoder‚Äìdecoder) with 1000 hidden units.  \n",
        "- **Attention types**:\n",
        "  - *Global attention*: soft alignment over all encoder states.  \n",
        "  - *Local attention*: predictive alignment chooses a window around position \\( p_t \\).  \n",
        "    - Local-m: monotonic assumption \\( p_t = t \\).  \n",
        "    - Local-p: predictive \\( p_t = S \\cdot \\sigma(v_p^\\top \\tanh(W_p h_t)) \\).  \n",
        "- **Input-feeding**: attentional vectors \\( \\tilde{h}_t \\) are fed into the next time step.  \n",
        "- **Alignment functions**:\n",
        "  - Dot: \\( \\text{score}(h_t, \\bar{h}_s) = h_t^\\top \\bar{h}_s \\).  \n",
        "  - General: \\( h_t^\\top W_a \\bar{h}_s \\).  \n",
        "  - Concat: \\( v_a^\\top \\tanh(W_a [h_t; \\bar{h}_s]) \\).  \n",
        "  - Location-based: \\( a_t = \\text{softmax}(W_a h_t) \\).  \n",
        "- **Training**: WMT‚Äô14 English‚ÄìGerman (4.5M pairs, 50K vocab), dropout regularization, SGD.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Results\n",
        "- **English ‚Üí German (WMT‚Äô14)**:\n",
        "  - +1.3 BLEU from reversing source sequence.  \n",
        "  - +1.4 BLEU from dropout.  \n",
        "  - +2.8 BLEU from global attention.  \n",
        "  - +1.3 BLEU from input feeding.  \n",
        "  - +0.9 BLEU from local-p attention.  \n",
        "  - +1.9 BLEU from <unk> replacement.  \n",
        "  - Final **ensemble**: **23.0 BLEU** vs. 21.6 of prior best system.  \n",
        "\n",
        "- **English ‚Üí German (WMT‚Äô15)**:\n",
        "  - Final ensemble with <unk> replacement achieves **25.9 BLEU**, new **state-of-the-art**, +1.0 BLEU over the best NMT + 5-gram rerank baseline.  \n",
        "\n",
        "- **German ‚Üí English (WMT‚Äô15)**:\n",
        "  - Global attention +2.2 BLEU.  \n",
        "  - Input feeding +1.0 BLEU.  \n",
        "  - Dot-product + dropout + feed + unk replacement ‚Üí **24.9 BLEU**.  \n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Conclusion\n",
        "- **Both global and local attention improve NMT**, with local-p offering strong efficiency‚Äìaccuracy tradeoffs.  \n",
        "- **Input feeding** helps maintain alignment coverage.  \n",
        "- **Different alignment functions matter**: dot/general outperform concat/location.  \n",
        "- Attention mechanisms are especially beneficial for **handling long sentences** and **rare word translation**.  \n",
        "- With ensembles, these approaches set **new SOTA results** on WMT‚Äô14/15 English‚ÄìGerman tasks.  \n",
        "\n",
        "This work demonstrates that **carefully designed attention mechanisms are critical to advancing neural machine translation**, directly bridging Bahdanau‚Äôs additive attention and the Q‚ÄìK‚ÄìV framework later popularized in Transformers.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "DAJeqj35nqBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 0. Imports =====\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ],
      "metadata": {
        "id": "cEQbN_VNoQcX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RqDbGfX4noSa"
      },
      "outputs": [],
      "source": [
        "class DecoderWithAttention(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, attention):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.LSTM(embed_size + hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size*2, vocab_size)\n",
        "        self.attention = attention\n",
        "\n",
        "    def forward(self, x, hidden, encoder_outputs, prev_context):\n",
        "        x = x.unsqueeze(1)\n",
        "        emb = self.embedding(x)\n",
        "        # Attention\n",
        "        context, attn_weights = self.attention(hidden[0].squeeze(0), encoder_outputs)\n",
        "        rnn_input = torch.cat((emb, context.unsqueeze(1)), dim=2)\n",
        "        output, hidden = self.rnn(rnn_input, hidden)\n",
        "        output = output.squeeze(1)\n",
        "        pred = self.fc(torch.cat((output, context), dim=1))\n",
        "        return pred, hidden, context, attn_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Small English‚ÜíFrench dataset\n",
        "pairs = [\n",
        "    (\"i am a student\", \"je suis un etudiant\"),\n",
        "    (\"he likes apples\", \"il aime les pommes\"),\n",
        "    (\"she loves music\", \"elle aime la musique\"),\n",
        "    (\"we are friends\", \"nous sommes amis\"),\n",
        "    (\"they play football\", \"ils jouent au football\")\n",
        "]\n",
        "\n",
        "# Build vocab\n",
        "def build_vocab(sentences):\n",
        "    tokens = set()\n",
        "    for s in sentences:\n",
        "        tokens.update(s.split())\n",
        "    vocab = {tok: idx+4 for idx, tok in enumerate(sorted(tokens))}\n",
        "    vocab[\"<pad>\"] = 0\n",
        "    vocab[\"<sos>\"] = 1\n",
        "    vocab[\"<eos>\"] = 2\n",
        "    vocab[\"<unk>\"] = 3\n",
        "    return vocab\n",
        "\n",
        "src_vocab = build_vocab([src for src, _ in pairs])\n",
        "trg_vocab = build_vocab([trg for _, trg in pairs])\n",
        "inv_trg_vocab = {i: t for t, i in trg_vocab.items()}\n",
        "\n",
        "def encode(sentence, vocab, max_len=8):\n",
        "    tokens = sentence.split()\n",
        "    idxs = [vocab[\"<sos>\"]] + [vocab.get(t, vocab[\"<unk>\"]) for t in tokens] + [vocab[\"<eos>\"]]\n",
        "    idxs += [vocab[\"<pad>\"]] * (max_len - len(idxs))\n",
        "    return torch.tensor(idxs)\n",
        "\n",
        "src_tensors = torch.stack([encode(src, src_vocab) for src, _ in pairs])\n",
        "trg_tensors = torch.stack([encode(trg, trg_vocab) for _, trg in pairs])\n"
      ],
      "metadata": {
        "id": "w-8v0VumoarI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.LSTM(embed_size, hidden_size, batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        outputs, (hidden, cell) = self.rnn(emb)\n",
        "        return outputs, (hidden, cell)\n"
      ],
      "metadata": {
        "id": "HeCv4mM_ofDV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Global Attention =====\n",
        "class GlobalAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(hidden_size*2, hidden_size, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # Project encoder outputs down from 2H ‚Üí H\n",
        "        encoder_proj = self.linear(encoder_outputs)   # [B,T,H]\n",
        "        scores = torch.bmm(encoder_proj, hidden.unsqueeze(2)).squeeze(2) # [B,T]\n",
        "        attn_weights = torch.softmax(scores, dim=1)\n",
        "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_proj).squeeze(1)\n",
        "        return context, attn_weights\n",
        "\n",
        "# ===== Local Attention (predictive) =====\n",
        "class LocalAttention(nn.Module):\n",
        "    def __init__(self, hidden_size, window=3):\n",
        "        super().__init__()\n",
        "        self.Wp = nn.Linear(hidden_size, hidden_size)\n",
        "        self.vp = nn.Linear(hidden_size, 1)\n",
        "        self.window = window\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        B, T, H = encoder_outputs.size()\n",
        "        # predict alignment position p_t\n",
        "        p_t = T * torch.sigmoid(self.vp(torch.tanh(self.Wp(hidden)))).squeeze(1)\n",
        "        contexts, weights = [], []\n",
        "        for b in range(B):\n",
        "            p = int(p_t[b].item())\n",
        "            left, right = max(0,p-self.window), min(T,p+self.window)\n",
        "            window = encoder_outputs[b,left:right,:]\n",
        "            scores = torch.matmul(window, hidden[b])\n",
        "            w = torch.softmax(scores, dim=0)\n",
        "            c = torch.matmul(w, window)\n",
        "            contexts.append(c)\n",
        "            weights.append(w)\n",
        "        return torch.stack(contexts), weights\n"
      ],
      "metadata": {
        "id": "M4gNQn79ohu1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderWithAttention(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, attention):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        # use embed_size + hidden_size, not hidden_size*2\n",
        "        self.rnn = nn.LSTM(embed_size + hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size*2, vocab_size)  # combine [output, context]\n",
        "        self.attention = attention\n",
        "\n",
        "    def forward(self, x, hidden, encoder_outputs, prev_context):\n",
        "        x = x.unsqueeze(1)\n",
        "        emb = self.embedding(x)\n",
        "        context, attn_weights = self.attention(hidden[0].squeeze(0), encoder_outputs)\n",
        "        rnn_input = torch.cat((emb, context.unsqueeze(1)), dim=2)\n",
        "        output, hidden = self.rnn(rnn_input, hidden)\n",
        "        pred = self.fc(torch.cat((output.squeeze(1), context), dim=1))\n",
        "        return pred, hidden, context, attn_weights\n"
      ],
      "metadata": {
        "id": "FJqnZRqPolJ7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder, self.decoder, self.device = encoder, decoder, device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing=0.5):\n",
        "        B, T = trg.size()\n",
        "        vocab_size = self.decoder.fc.out_features\n",
        "        outputs = torch.zeros(B, T, vocab_size).to(self.device)\n",
        "\n",
        "        enc_out, hidden = self.encoder(src)\n",
        "        hidden = (hidden[0][::2] + hidden[0][1::2], hidden[1][::2] + hidden[1][1::2]) # combine BiLSTM\n",
        "\n",
        "        input = trg[:,0]\n",
        "        context = torch.zeros(B, hidden[0].size(1)).to(self.device)\n",
        "\n",
        "        for t in range(1,T):\n",
        "            out, hidden, context, _ = self.decoder(input, hidden, enc_out, context)\n",
        "            outputs[:,t,:] = out\n",
        "            input = trg[:,t] if torch.rand(1).item() < teacher_forcing else out.argmax(1)\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "Ng4uQIbhoocf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size, hidden_size = 32, 64\n",
        "attention = GlobalAttention(hidden_size)   # swap with LocalAttention(hidden_size)\n",
        "encoder = Encoder(len(src_vocab)+1, embed_size, hidden_size).to(device)\n",
        "decoder = DecoderWithAttention(len(trg_vocab)+1, embed_size, hidden_size, attention).to(device)\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "opt = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "src_tensors, trg_tensors = src_tensors.to(device), trg_tensors.to(device)\n",
        "\n",
        "for epoch in range(200):\n",
        "    model.train()\n",
        "    opt.zero_grad()\n",
        "    out = model(src_tensors, trg_tensors)\n",
        "    out_dim = out.shape[-1]\n",
        "    loss = criterion(out[:,1:,:].reshape(-1, out_dim), trg_tensors[:,1:].reshape(-1))\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    if (epoch+1)%50==0:\n",
        "        print(f\"Epoch {epoch+1}/200 | Loss {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-cX8TYZoruR",
        "outputId": "1e625ca3-89cf-47eb-d048-805be85da73b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/200 | Loss 0.0053\n",
            "Epoch 100/200 | Loss 0.0014\n",
            "Epoch 150/200 | Loss 0.0009\n",
            "Epoch 200/200 | Loss 0.0006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence, model, src_vocab, trg_vocab, inv_trg_vocab, max_len=8):\n",
        "    model.eval()\n",
        "    src = encode(sentence, src_vocab).unsqueeze(0).to(device)\n",
        "    trg_idx = [trg_vocab[\"<sos>\"]]\n",
        "    enc_out, hidden = model.encoder(src)\n",
        "    hidden = (hidden[0][::2]+hidden[0][1::2], hidden[1][::2]+hidden[1][1::2])\n",
        "    context = torch.zeros(1, hidden[0].size(1)).to(device)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        x = torch.tensor([trg_idx[-1]]).to(device)\n",
        "        out, hidden, context, _ = model.decoder(x, hidden, enc_out, context)\n",
        "        pred = out.argmax(1).item()\n",
        "        trg_idx.append(pred)\n",
        "        if pred == trg_vocab[\"<eos>\"]: break\n",
        "\n",
        "    return \" \".join([inv_trg_vocab[i] for i in trg_idx if i not in (0,1,2,3)])\n",
        "\n",
        "print(translate(\"i am a student\", model, src_vocab, trg_vocab, inv_trg_vocab))\n",
        "print(translate(\"she loves music\", model, src_vocab, trg_vocab, inv_trg_vocab))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6_WMDofovfq",
        "outputId": "3099d118-ada7-412f-ed01-c5f11e0756e9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "je suis un etudiant\n",
            "elle aime la musique\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîé Evolution of Attention: Bahdanau (2014) ‚Üí Luong (2015) ‚Üí Transformer (2017)\n",
        "\n",
        "| Aspect | Bahdanau et al. (2014) ‚Äì Additive Attention | Luong et al. (2015) ‚Äì Multiplicative Attention | Vaswani et al. (2017) ‚Äì Scaled Dot-Product Attention (Transformer) | Why it matters |\n",
        "|---|---|---|---|---|\n",
        "| **Problem Addressed** | Fixed-length bottleneck in seq2seq translation | Efficiency and design choices in NMT attention | Full parallelization and long-range dependencies in sequences | Each step builds toward scalable and general attention |\n",
        "| **Attention Type** | *Additive*: feed-forward MLP computes alignment | *Multiplicative*: dot/general/concat scoring; Global vs Local | *Scaled dot-product*: multi-head self-attention | Transformers unify and generalize earlier ideas |\n",
        "| **Score Function** | $$ e_{ij} = v^\\top \\tanh\\!\\big(W [s_{i-1}; h_j]\\big) $$ | Global (dot): $$ \\text{score}(h_t,\\bar{h}_s) = h_t^\\top \\bar{h}_s $$ Local-p (predictive): $$ p_t = S \\cdot \\sigma(v_p^\\top \\tanh(W_p h_t)) $$ | $$ \\text{score}(Q,K) = \\frac{QK^\\top}{\\sqrt{d_k}} $$ | Bahdanau = additive MLP, Luong = efficient multiplicative, Transformer = normalized dot-product |\n",
        "| **Weights** | $$ \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_k \\exp(e_{ik})} $$ | $$ \\alpha_t(s) = \\text{softmax}(\\text{score}(h_t,\\bar{h}_s)) $$ | $$ \\alpha = \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) $$ | All normalize via softmax, but Transformers compute in parallel for all tokens |\n",
        "| **Context Vector** | $$ c_i = \\sum_j \\alpha_{ij} h_j $$ | $$ c_t = \\sum_s \\alpha_t(s) \\bar{h}_s $$ | $$ \\text{Attention}(Q,K,V) = \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V $$ | Context always = weighted sum; formulation becomes more general |\n",
        "| **Inputs (Q,K,V analogy)** | Query = decoder hidden $s_{i-1}$; Keys/Values = encoder states $h_j$ | Query = decoder hidden $h_t$; Keys/Values = encoder states | Queries, Keys, Values all derived from embeddings via linear projections | Transformer formalizes Q‚ÄìK‚ÄìV explicitly, enabling self-attention |\n",
        "| **Architecture** | Encoder: BiRNN (GRU), Decoder: GRU | Encoder‚ÄìDecoder: stacked LSTMs (4√ó1000 units) | Encoder‚ÄìDecoder: multi-head self-attention + FFN layers | Transformer removes recurrence, relies purely on attention |\n",
        "| **Extra Techniques** | First to visualize attention alignments | Input-feeding; local/global modes; <unk> replacement | Multi-head attention; positional encoding; residuals & normalization | Stepwise refinements toward GPT-scale architectures |\n",
        "| **Results** | Better BLEU vs vanilla seq2seq; big gains on long sentences | +5 BLEU over baselines; new SOTA on WMT‚Äô15 English‚ÄìGerman | New state-of-the-art across tasks; foundation for GPT/BERT | Each stage pushed NMT and sequence modeling forward |\n",
        "| **Impact** | Birth of attention mechanism | Practical recipes for effective NMT attention | Foundation of modern LLMs (GPT, BERT, etc.) | Historical lineage: Bahdanau ‚Üí Luong ‚Üí Transformer |\n"
      ],
      "metadata": {
        "id": "XQZJF16hp9US"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå Summary\n",
        "\n",
        "Bahdanau (2014): Introduced additive attention ‚Äî decoder looks back at encoder states via an alignment model.\n",
        "\n",
        "Luong (2015): Simplified to multiplicative scoring (dot/general), added global/local modes and input feeding, scaled to WMT benchmarks.\n",
        "\n",
        "Vaswani (2017): Formalized attention into Q‚ÄìK‚ÄìV framework, introduced multi-head scaled dot-product attention, enabling Transformers and GPT."
      ],
      "metadata": {
        "id": "x6d-HRbBqpji"
      }
    }
  ]
}