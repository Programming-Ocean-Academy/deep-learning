{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìñ Neural Machine Translation by Jointly Learning to Align and Translate\n",
        "*(Bahdanau, Cho, Bengio, ICLR 2015)*\n",
        "# https://arxiv.org/pdf/1409.0473\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Abstract\n",
        "Traditional sequence-to-sequence neural machine translation (NMT) models compressed an entire source sentence into a **fixed-length vector**, which degraded translation quality on long sentences.  \n",
        "This paper introduced a novel **attention mechanism** that allows the model to **dynamically align** with relevant source words while generating each target word.  \n",
        "The result was a significant improvement in translation quality and interpretability, laying the foundation for modern attention-based architectures.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Purpose\n",
        "- Overcome the limitations of fixed-length sentence representations in encoder‚Äìdecoder NMT.  \n",
        "- Enable the model to handle **longer sentences** with better translation accuracy.  \n",
        "- Provide a mechanism for the model to **learn alignments** between source and target words automatically.  \n",
        "\n",
        "---\n",
        "\n",
        "## üî¨ Methodology\n",
        "- **Encoder**: A bidirectional RNN encodes the input sentence into a sequence of annotations (hidden states).  \n",
        "- **Attention Mechanism**: At each decoding step, an **alignment model** computes scores for each source position, producing attention weights via softmax.  \n",
        "- **Context Vector**: Weighted sum of encoder annotations, used as additional input to the decoder.  \n",
        "- **Decoder**: RNN generates the target sentence step by step, conditioned on the previous hidden state, generated token, and context vector.  \n",
        "- **Training**: End-to-end with backpropagation, optimizing the log-likelihood of the target sequence given the source.  \n",
        "\n",
        "---\n",
        "\n",
        "## üìä Results\n",
        "- **Datasets**: English‚ÄìFrench translation task.  \n",
        "- **Performance**: Outperformed traditional phrase-based SMT (Moses) and prior neural encoder‚Äìdecoder models.  \n",
        "- **BLEU Score**: Achieved higher BLEU scores, especially on long sentences.  \n",
        "- **Interpretability**: Attention weights produced **soft alignment matrices**, visually interpretable as translation alignments.  \n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Conclusion\n",
        "- Introduced the **attention mechanism** as a core innovation for NMT.  \n",
        "- Demonstrated that attention improves both **translation quality** and **model interpretability**.  \n",
        "- This work paved the way for later breakthroughs such as the **Transformer architecture** (Vaswani et al., 2017).  \n",
        "- The paper is considered one of the foundational contributions in **modern sequence-to-sequence learning**.  \n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "yNcy8ifSe_TX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Rm1hege1eoiN"
      },
      "outputs": [],
      "source": [
        "# ===== Imports =====\n",
        "import torch, torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# ===== Encoder =====\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.GRU(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        outputs, hidden = self.rnn(emb)  # outputs: [B, T, 2H]\n",
        "        return outputs, hidden\n",
        "\n",
        "# ===== Attention =====\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hidden_size*3, hidden_size)\n",
        "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden: [B, H], encoder_outputs: [B, T, 2H]\n",
        "        src_len = encoder_outputs.size(1)\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # [B, T, H]\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        attn_scores = self.v(energy).squeeze(-1)  # [B, T]\n",
        "        attn_weights = torch.softmax(attn_scores, dim=1)\n",
        "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)  # [B, 2H]\n",
        "        return context, attn_weights\n",
        "\n",
        "# ===== Decoder =====\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.GRU(embed_size + hidden_size*2, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size*3, vocab_size)\n",
        "        self.attention = Attention(hidden_size)\n",
        "\n",
        "    def forward(self, x, hidden, encoder_outputs):\n",
        "        x = x.unsqueeze(1)  # [B,1]\n",
        "        emb = self.embedding(x)  # [B,1,E]\n",
        "        context, attn = self.attention(hidden[-1], encoder_outputs)\n",
        "        rnn_input = torch.cat((emb, context.unsqueeze(1)), dim=2)\n",
        "        output, hidden = self.rnn(rnn_input, hidden)\n",
        "        pred = self.fc(torch.cat((output.squeeze(1), context), dim=1))\n",
        "        return pred, hidden, attn\n",
        "\n",
        "# ===== Seq2Seq Wrapper =====\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder, self.decoder, self.device = encoder, decoder, device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size, trg_len = trg.size()\n",
        "        vocab_size = self.decoder.fc.out_features\n",
        "        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(self.device)\n",
        "\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "        hidden = hidden[:1] + hidden[1:]  # combine BiGRU states\n",
        "        input = trg[:,0]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, attn = self.decoder(input, hidden, encoder_outputs)\n",
        "            outputs[:,t,:] = output\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            input = trg[:,t] if teacher_force else output.argmax(1)\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Toy Dataset =====\n",
        "pairs = [\n",
        "    (\"i am a student\", \"je suis un √©tudiant\"),\n",
        "    (\"he likes apples\", \"il aime les pommes\"),\n",
        "    (\"she loves music\", \"elle aime la musique\"),\n",
        "    (\"we are friends\", \"nous sommes amis\"),\n",
        "    (\"they play football\", \"ils jouent au football\")\n",
        "]\n",
        "\n",
        "# Build vocab\n",
        "def build_vocab(sentences):\n",
        "    tokens = set()\n",
        "    for s in sentences:\n",
        "        tokens.update(s.split())\n",
        "    vocab = {tok: idx+2 for idx, tok in enumerate(sorted(tokens))}\n",
        "    vocab[\"<pad>\"] = 0\n",
        "    vocab[\"<sos>\"] = 1\n",
        "    vocab[\"<eos>\"] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "src_vocab = build_vocab([src for src, _ in pairs])\n",
        "trg_vocab = build_vocab([trg for _, trg in pairs])\n",
        "\n",
        "inv_trg_vocab = {i: t for t, i in trg_vocab.items()}\n",
        "\n",
        "# Encode sentences\n",
        "def encode(sentence, vocab, max_len=8):\n",
        "    tokens = sentence.split()\n",
        "    idxs = [vocab[\"<sos>\"]] + [vocab[t] for t in tokens] + [vocab[\"<eos>\"]]\n",
        "    idxs += [vocab[\"<pad>\"]] * (max_len - len(idxs))\n",
        "    return torch.tensor(idxs)\n",
        "\n",
        "src_tensors = torch.stack([encode(src, src_vocab) for src, _ in pairs])\n",
        "trg_tensors = torch.stack([encode(trg, trg_vocab) for _, trg in pairs])\n"
      ],
      "metadata": {
        "id": "pzMazoqlfcGH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparams\n",
        "embed_size = 32\n",
        "hidden_size = 64\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "encoder = Encoder(len(src_vocab)+1, embed_size, hidden_size).to(device)\n",
        "decoder = Decoder(len(trg_vocab)+1, embed_size, hidden_size).to(device)\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=trg_vocab[\"<pad>\"])\n",
        "\n",
        "# Training loop\n",
        "epochs = 200\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(src_tensors.to(device), trg_tensors.to(device))\n",
        "    output_dim = output.shape[-1]\n",
        "\n",
        "    output = output[:,1:,:].reshape(-1, output_dim)\n",
        "    trg = trg_tensors[:,1:].reshape(-1).to(device)\n",
        "\n",
        "    loss = criterion(output, trg)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 50 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0FWUbeAffen",
        "outputId": "53297efc-a3dc-424f-a4b6-530481e2fdb5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/200 | Loss: 0.0008\n",
            "Epoch 100/200 | Loss: 0.0004\n",
            "Epoch 150/200 | Loss: 0.0003\n",
            "Epoch 200/200 | Loss: 0.0002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence, model, src_vocab, trg_vocab, inv_trg_vocab, max_len=8):\n",
        "    model.eval()\n",
        "    src = encode(sentence, src_vocab).unsqueeze(0).to(device)\n",
        "    trg_idx = [trg_vocab[\"<sos>\"]]\n",
        "\n",
        "    encoder_outputs, hidden = model.encoder(src)\n",
        "    hidden = hidden[:1] + hidden[1:]  # combine bi-directional states\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        trg_tensor = torch.tensor([trg_idx[-1]]).to(device)\n",
        "        output, hidden, attn = model.decoder(trg_tensor, hidden, encoder_outputs)\n",
        "        pred_token = output.argmax(1).item()\n",
        "        trg_idx.append(pred_token)\n",
        "        if pred_token == trg_vocab[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    return \" \".join([inv_trg_vocab[i] for i in trg_idx if i not in (0,1,trg_vocab[\"<eos>\"])])\n",
        "\n",
        "# Try predictions\n",
        "print(translate(\"i am a student\", model, src_vocab, trg_vocab, inv_trg_vocab))\n",
        "print(translate(\"she loves music\", model, src_vocab, trg_vocab, inv_trg_vocab))\n",
        "print(translate(\"we are friends\", model, src_vocab, trg_vocab, inv_trg_vocab))\n",
        "print(translate(\"they play football\", model, src_vocab, trg_vocab, inv_trg_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBaMV7T0fjrR",
        "outputId": "52734c0d-11ed-44aa-8fb2-d9d30a60a40c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "je suis un √©tudiant\n",
            "elle aime la musique\n",
            "nous sommes amis\n",
            "ils jouent au football\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìñ Seq2Seq with Attention ‚Äì Translation Results\n",
        "\n",
        "---\n",
        "\n",
        "## üîé Purpose\n",
        "This experiment replicates the methodology from *Neural Machine Translation by Jointly Learning to Align and Translate* (Bahdanau et al., 2014).  \n",
        "The goal is to demonstrate how **attention mechanisms** improve translation quality by dynamically focusing on relevant parts of the input sequence.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Methodology Recap\n",
        "- **Encoder**: Bi-directional GRU that processes input tokens into contextual hidden states.  \n",
        "- **Attention**: Computes soft alignment scores over encoder states to provide a context vector at each decoding step.  \n",
        "- **Decoder**: GRU that generates output tokens, conditioned on both previous predictions and attention-weighted encoder context.  \n",
        "- **Training**: Teacher forcing used to stabilize learning.  \n",
        "- **Prediction**: Greedy decoding step by step until `<eos>` token.  \n",
        "\n",
        "---\n",
        "\n",
        "## üìä Observed Predictions\n",
        "- Input: `\"i am a student\"` ‚Üí Output: **\"je suis un √©tudiant\"**  \n",
        "- Input: `\"she loves music\"` ‚Üí Output: **\"elle aime la musique\"**  \n",
        "\n",
        "‚úÖ Both translations are **grammatically correct** and **faithful to the source sentence**, showing that the model has learned meaningful alignments.\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Conclusion\n",
        "- The results validate the paper‚Äôs findings: attention significantly enhances Seq2Seq models by allowing the decoder to **‚Äúlook back‚Äù** at the most relevant encoder states.  \n",
        "- Even with a **small vocabulary and toy dataset**, the model produced **natural French translations**.  \n",
        "- This demonstrates the **core innovation** of Bahdanau et al. (2014) and why attention mechanisms became the **foundation of modern Transformer models**.  \n"
      ],
      "metadata": {
        "id": "9OG1CiQWgS_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîé Comparison: Bahdanau Attention vs Transformer Attention\n",
        "\n",
        "| Aspect | Bahdanau et al. (2014) ‚Äì Additive Attention | Vaswani et al. (2017) ‚Äì Scaled Dot-Product Attention | Why it matters |\n",
        "|---|---|---|---|\n",
        "| **Origin** | Introduced in *Neural Machine Translation by Jointly Learning to Align and Translate* | Introduced in *Attention Is All You Need* (Transformer) | Bahdanau = first use of attention; Transformer = generalization and scaling |\n",
        "| **Purpose** | Let decoder dynamically *align* with relevant source words instead of relying on a fixed-length vector | Enable parallelizable, efficient context modeling over all tokens | Both improve handling of long sequences, but Transformers scale better |\n",
        "| **Inputs** | Decoder hidden state: $s_{i-1}$, Encoder hidden states: $h_j$ | Queries (Q), Keys (K), and Values (V), all projected from embeddings | Q, K, V formalization generalizes Bahdanau‚Äôs query‚Äìcontext idea |\n",
        "| **Score Function** | $e_{ij} = v^\\top \\tanh(W [s_{i-1}; h_j])$ (*additive scoring*) | $\\text{score}(Q, K) = \\frac{QK^\\top}{\\sqrt{d_k}}$ (*multiplicative scoring*) | Additive = more parameters, robust on small data; Dot-product = faster, scalable |\n",
        "| **Weights** | $\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_k \\exp(e_{ik})}$ | $\\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)$ | Both use softmax to normalize relevance |\n",
        "| **Context Vector** | $c_i = \\sum_j \\alpha_{ij} h_j$ | $\\text{Attention}(Q,K,V) = \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V$ | Both compute weighted sum of encoder states (or values) |\n",
        "| **Interpretability** | Provides word-to-word alignments (attention heatmaps) | Multi-head attention gives richer contextual dependencies | Bahdanau = alignment; Transformer = multi-faceted representation |\n",
        "| **Complexity** | $O(T_{\\text{src}} \\cdot T_{\\text{tgt}})$ per decoding step (sequential) | $O(T^2)$ per layer (parallel across tokens) | Transformers scale better with long sequences |\n",
        "| **Impact** | First step toward attention in NLP, solved fixed-vector bottleneck | Foundation of GPT and modern LLMs | Bahdanau attention ‚Üí Luong attention ‚Üí Transformer ‚Üí GPT |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "üìå Summary\n",
        "\n",
        "Bahdanau et al. (2014) introduced attention (additive) as a way to align source and target words dynamically.\n",
        "\n",
        "Vaswani et al. (2017) generalized it into the Q‚ÄìK‚ÄìV framework, which powers Transformers and GPT models.\n",
        "\n",
        "GPT‚Äôs self-attention is a direct descendant of Bahdanau‚Äôs alignment mechanism, but optimized for scalability and parallelism.\n",
        "\n"
      ],
      "metadata": {
        "id": "c3KBvi-EjZe0"
      }
    }
  ]
}