{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN8MEjEkgGeQCyC9/CMATtm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":532},"id":"_MWRiA4MgDHR","executionInfo":{"status":"error","timestamp":1754687905740,"user_tz":-480,"elapsed":556614,"user":{"displayName":"Programming Ocean Academy","userId":"12517642345024321372"}},"outputId":"5bb56caa-fe8b-462d-dc44-2a3b18e1797b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Trainable params: 37.41M\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-718079632.py:419: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)\n","100%|██████████| 170M/170M [01:51<00:00, 1.53MB/s]\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","/tmp/ipython-input-718079632.py:436: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=args.amp):\n"]},{"output_type":"stream","name":"stdout","text":["[epoch 0 step 0] loss: 1.1330\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-718079632.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# args now has .seed and all other attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-718079632.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiffusion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-718079632.py\u001b[0m in \u001b[0;36mtraining_losses\u001b[0;34m(self, x0)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mx_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0meps_theta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps_theta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-718079632.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-718079632.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mh_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mqkv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqkv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqkv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import math, os, argparse, random, itertools, time\n","from dataclasses import dataclass\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms, utils as vutils\n","\n","# ---------------------------\n","# Utilities\n","# ---------------------------\n","\n","def set_seed(seed=42):\n","    random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n","\n","def exists(x): return x is not None\n","\n","def default(val, d): return val if exists(val) else d\n","\n","def count_params(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","def save_image_grid(tensor, path, nrow=8, normalize=True, value_range=(-1,1)):\n","    # tensor in [-1,1] -> [0,1]\n","    if normalize:\n","        lo, hi = value_range\n","        tensor = (tensor - lo) / (hi - lo)\n","        tensor = tensor.clamp(0, 1)\n","    vutils.save_image(tensor, path, nrow=nrow)\n","\n","# ---------------------------\n","# Sinusoidal timestep embeddings (Transformer-style)\n","# ---------------------------\n","\n","def timestep_embedding(timesteps, dim, max_period=10000):\n","    \"\"\"\n","    timesteps: (batch,) with values in [0, T-1]\n","    returns: (batch, dim)\n","    \"\"\"\n","    half = dim // 2\n","    freqs = torch.exp(-math.log(max_period) * torch.arange(0, half, dtype=torch.float32, device=timesteps.device) / half)\n","    args = timesteps.float().unsqueeze(1) * freqs.unsqueeze(0)\n","    emb = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n","    if dim % 2:\n","        emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=-1)\n","    return emb\n","\n","# ---------------------------\n","# Building blocks: ResBlock, Attention, Down/Up\n","# ---------------------------\n","\n","class SiLU(nn.Module):\n","    def forward(self, x): return x * torch.sigmoid(x)\n","\n","class TimeMLP(nn.Module):\n","    def __init__(self, time_dim, out_dim):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(time_dim, out_dim),\n","            SiLU(),\n","            nn.Linear(out_dim, out_dim),\n","        )\n","    def forward(self, t_emb): return self.net(t_emb)\n","\n","class ResBlock(nn.Module):\n","    def __init__(self, in_ch, out_ch, time_dim, dropout=0.0):\n","        super().__init__()\n","        self.norm1 = nn.GroupNorm(32, in_ch)\n","        self.act1  = SiLU()\n","        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n","\n","        self.time = TimeMLP(time_dim, out_ch)\n","\n","        self.norm2 = nn.GroupNorm(32, out_ch)\n","        self.act2  = SiLU()\n","        self.dropout = nn.Dropout(dropout)\n","        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n","\n","        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n","\n","    def forward(self, x, t_emb):\n","        h = self.conv1(self.act1(self.norm1(x)))\n","        h = h + self.time(t_emb)[:, :, None, None]\n","        h = self.conv2(self.dropout(self.act2(self.norm2(h))))\n","        return h + self.skip(x)\n","\n","class SelfAttention2d(nn.Module):\n","    def __init__(self, channels, num_heads=4):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        self.norm = nn.GroupNorm(32, channels)\n","        self.qkv = nn.Conv2d(channels, channels * 3, 1)\n","        self.proj = nn.Conv2d(channels, channels, 1)\n","\n","    def forward(self, x):\n","        b, c, h, w = x.shape\n","        h_ = self.norm(x)\n","        qkv = self.qkv(h_)\n","        q, k, v = qkv.chunk(3, dim=1)\n","\n","        # reshape to (b, heads, dim, hw)\n","        heads = self.num_heads\n","        dim_head = c // heads\n","        q = q.view(b, heads, dim_head, h*w)\n","        k = k.view(b, heads, dim_head, h*w)\n","        v = v.view(b, heads, dim_head, h*w)\n","\n","        attn = torch.einsum('bhdi,bhdj->bhij', q, k) / math.sqrt(dim_head)\n","        attn = attn.softmax(dim=-1)\n","        out = torch.einsum('bhij,bhdj->bhdi', attn, v)\n","        out = out.reshape(b, c, h, w)\n","        return x + self.proj(out)\n","\n","class Downsample(nn.Module):\n","    def __init__(self, ch_in, ch_out):\n","        super().__init__()\n","        self.conv = nn.Conv2d(ch_in, ch_out, 3, stride=2, padding=1)\n","    def forward(self, x): return self.conv(x)\n","\n","class Upsample(nn.Module):\n","    def __init__(self, ch_in, ch_out):\n","        super().__init__()\n","        self.conv = nn.Conv2d(ch_in, ch_out, 3, padding=1)\n","    def forward(self, x):\n","        x = F.interpolate(x, scale_factor=2, mode='nearest')\n","        return self.conv(x)\n","\n","# ---------------------------\n","# U-Net backbone (PixelCNN++ style, with attention at 16x16)\n","# ---------------------------\n","\n","class UNet(nn.Module):\n","    def __init__(\n","        self,\n","        in_channels=3,\n","        base_channels=128,\n","        channel_mults=(1, 2, 2, 2),  # resolutions: 32,16,8,4\n","        num_res_blocks=2,\n","        time_dim=512,\n","        dropout=0.1,\n","        attn_resolutions=(16,),  # add attention at 16x16 as in the paper\n","        img_size=32\n","    ):\n","        super().__init__()\n","        self.img_size = img_size\n","        self.time_mlp = nn.Sequential(\n","            nn.Linear(time_dim, time_dim),\n","            SiLU(),\n","            nn.Linear(time_dim, time_dim),\n","        )\n","\n","        self.time_in = nn.Linear(time_dim//2*2, time_dim)  # we pass sinusoidal, then project\n","\n","        # input conv\n","        self.in_conv = nn.Conv2d(in_channels, base_channels, 3, padding=1)\n","\n","        # Down path\n","        in_ch = base_channels\n","        self.downs = nn.ModuleList()\n","        self.downs_attn = nn.ModuleList()\n","        hs = [in_ch]\n","        curr_res = img_size\n","        for i, mult in enumerate(channel_mults):\n","            out_ch = base_channels * mult\n","            for _ in range(num_res_blocks):\n","                self.downs.append(ResBlock(in_ch, out_ch, time_dim, dropout))\n","                in_ch = out_ch\n","                hs.append(in_ch)\n","                # attention at chosen resolution\n","                self.downs_attn.append(SelfAttention2d(in_ch) if curr_res in attn_resolutions else nn.Identity())\n","            if i != len(channel_mults) - 1:\n","                self.downs.append(Downsample(in_ch, in_ch))\n","                self.downs_attn.append(nn.Identity())\n","                curr_res //= 2\n","                hs.append(in_ch)\n","\n","        # Middle\n","        self.mid_block1 = ResBlock(in_ch, in_ch, time_dim, dropout)\n","        self.mid_attn = SelfAttention2d(in_ch)\n","        self.mid_block2 = ResBlock(in_ch, in_ch, time_dim, dropout)\n","\n","        # Up path\n","        self.ups = nn.ModuleList()\n","        self.ups_attn = nn.ModuleList()\n","        for i, mult in reversed(list(enumerate(channel_mults))):\n","            out_ch = base_channels * mult\n","            for _ in range(num_res_blocks + 1):  # +1 to consume skip from either res or downsample\n","                skip_ch = hs.pop()\n","                self.ups.append(ResBlock(in_ch + skip_ch, out_ch, time_dim, dropout))\n","                in_ch = out_ch\n","                self.ups_attn.append(SelfAttention2d(in_ch) if curr_res in attn_resolutions else nn.Identity())\n","            if i != 0:\n","                self.ups.append(Upsample(in_ch, in_ch))\n","                self.ups_attn.append(nn.Identity())\n","                curr_res *= 2\n","\n","        assert len(hs) == 0\n","\n","        # Output\n","        self.out_norm = nn.GroupNorm(32, in_ch)\n","        self.out_act = SiLU()\n","        self.out_conv = nn.Conv2d(in_ch, in_channels, 3, padding=1)\n","\n","        self.time_dim = time_dim\n","\n","    def forward(self, x, t):\n","        # t: (b,) in [0, T-1]\n","        t_emb = timestep_embedding(t, self.time_dim)\n","        t_emb = self.time_mlp(self.time_in(t_emb))\n","\n","        x = self.in_conv(x)\n","        feats = [x]\n","\n","        it = iter(self.downs_attn)\n","        for mod in self.downs:\n","            x = mod(x, t_emb) if isinstance(mod, ResBlock) else mod(x)\n","            attn = next(it)\n","            x = attn(x)\n","            feats.append(x)\n","\n","        x = self.mid_block1(x, t_emb)\n","        x = self.mid_attn(x)\n","        x = self.mid_block2(x, t_emb)\n","\n","        it = iter(self.ups_attn)\n","        for mod in self.ups:\n","            if isinstance(mod, ResBlock):\n","                skip = feats.pop()\n","                x = torch.cat([x, skip], dim=1)\n","                x = mod(x, t_emb)\n","            else:\n","                x = mod(x)\n","            attn = next(it)\n","            x = attn(x)\n","\n","        x = self.out_conv(self.out_act(self.out_norm(x)))\n","        return x  # predicts epsilon\n","\n","# ---------------------------\n","# Diffusion process (DDPM)\n","# ---------------------------\n","\n","@dataclass\n","class DiffusionConfig:\n","    image_size: int = 32\n","    channels: int = 3\n","    timesteps: int = 1000\n","    beta_start: float = 1e-4\n","    beta_end: float = 2e-2\n","    variance_type: str = \"fixed_small\"  # \"fixed_small\" -> posterior (beta_tilde), \"fixed_large\" -> beta_t\n","\n","class DDPM(nn.Module):\n","    def __init__(self, model: nn.Module, config: DiffusionConfig):\n","        super().__init__()\n","        self.model = model\n","        self.config = config\n","\n","        T = config.timesteps\n","        betas = torch.linspace(config.beta_start, config.beta_end, T, dtype=torch.float32)\n","        alphas = 1.0 - betas\n","        alphas_cumprod = torch.cumprod(alphas, dim=0)\n","        alphas_cumprod_prev = torch.cat([torch.tensor([1.], dtype=torch.float32), alphas_cumprod[:-1]], dim=0)\n","\n","        # register buffers\n","        for name, tensor in {\n","            \"betas\": betas,\n","            \"alphas\": alphas,\n","            \"alphas_cumprod\": alphas_cumprod,\n","            \"alphas_cumprod_prev\": alphas_cumprod_prev,\n","            \"sqrt_alphas_cumprod\": torch.sqrt(alphas_cumprod),\n","            \"sqrt_one_minus_alphas_cumprod\": torch.sqrt(1. - alphas_cumprod),\n","            \"sqrt_recip_alphas\": torch.sqrt(1.0 / alphas),\n","        }.items():\n","            self.register_buffer(name, tensor)\n","\n","        # posterior variance beta_tilde\n","        posterior_variance = (betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)).clamp(min=1e-20)\n","        self.register_buffer(\"posterior_variance\", posterior_variance)\n","        self.register_buffer(\"posterior_log_variance_clipped\", torch.log(posterior_variance.clamp(min=1e-20)))\n","\n","        # mean coefficient helpers for p_theta\n","        self.register_buffer(\"posterior_mean_coef1\", (betas * torch.sqrt(alphas_cumprod_prev)) / (1. - alphas_cumprod))\n","        self.register_buffer(\"posterior_mean_coef2\", ((1. - alphas_cumprod_prev) * torch.sqrt(alphas)) / (1. - alphas_cumprod))\n","\n","    def q_sample(self, x0, t, noise=None):\n","        \"\"\"\n","        q(x_t | x0) = N(sqrt(alpha_bar_t) x0, (1-alpha_bar_t) I)\n","        \"\"\"\n","        if noise is None:\n","            noise = torch.randn_like(x0)\n","        sqrt_ab = self.sqrt_alphas_cumprod[t].view(-1, 1, 1, 1)\n","        sqrt_omb = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n","        return sqrt_ab * x0 + sqrt_omb * noise\n","\n","    def p_mean_variance(self, x_t, t):\n","        \"\"\"\n","        Use epsilon-prediction parameterization to compute mean.\n","        Optionally choose variance type.\n","        \"\"\"\n","        eps_theta = self.model(x_t, t)\n","        beta_t = self.betas[t].view(-1,1,1,1)\n","        sqrt_recip_alpha_t = self.sqrt_recip_alphas[t].view(-1,1,1,1)\n","        sqrt_one_minus_ab_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1,1,1,1)\n","\n","        # Eq. 11-style mean: mu = 1/sqrt(alpha_t) * (x_t - beta_t / sqrt(1-ab_t) * eps_theta)\n","        # Note: beta_t == (1 - alpha_t)\n","        model_mean = sqrt_recip_alpha_t * (x_t - beta_t / sqrt_one_minus_ab_t * eps_theta)\n","\n","        if self.config.variance_type == \"fixed_small\":\n","            # posterior variance (beta_tilde) as in many DDPM refs\n","            model_var = self.posterior_variance[t].view(-1,1,1,1)\n","            model_log_var = self.posterior_log_variance_clipped[t].view(-1,1,1,1)\n","        else:\n","            # \"fixed_large\": set variance to beta_t (upper bound choice)\n","            model_var = beta_t\n","            model_log_var = torch.log(beta_t)\n","\n","        return model_mean, model_var, model_log_var, eps_theta\n","\n","    @torch.no_grad()\n","    def p_sample(self, x_t, t):\n","        \"\"\"\n","        One reverse step: x_{t-1} ~ N(mean, var).\n","        For t == 0, return mean (z = 0).\n","        \"\"\"\n","        b = x_t.shape[0]\n","        model_mean, model_var, model_log_var, _ = self.p_mean_variance(x_t, t)\n","        if (t == 0).all():\n","            return model_mean\n","        z = torch.randn_like(x_t)\n","        return model_mean + (model_var ** 0.5) * z\n","\n","    @torch.no_grad()\n","    def sample(self, batch_size, device):\n","        x = torch.randn(batch_size, 3, self.config.image_size, self.config.image_size, device=device)\n","        for i in reversed(range(self.config.timesteps)):\n","            t = torch.full((batch_size,), i, device=device, dtype=torch.long)\n","            x = self.p_sample(x, t)\n","        return x\n","\n","    def training_losses(self, x0):\n","        \"\"\"\n","        L_simple: E_{t, eps} || eps - eps_theta(x_t, t) ||^2\n","        \"\"\"\n","        b = x0.size(0)\n","        t = torch.randint(0, self.config.timesteps, (b,), device=x0.device).long()\n","        noise = torch.randn_like(x0)\n","        x_t = self.q_sample(x0, t, noise)\n","        eps_theta = self.model(x_t, t)\n","        loss = F.mse_loss(eps_theta, noise, reduction='mean')\n","        return loss\n","\n","# ---------------------------\n","# EMA (as in the paper)\n","# ---------------------------\n","\n","class EMA:\n","    def __init__(self, model, decay=0.9999):\n","        self.decay = decay\n","        self.shadow = {k: v.clone().detach() for k, v in model.state_dict().items() if v.dtype.is_floating_point}\n","    def update(self, model):\n","        with torch.no_grad():\n","            for k, v in model.state_dict().items():\n","                if k in self.shadow and v.dtype.is_floating_point:\n","                    self.shadow[k].mul_(self.decay).add_(v, alpha=1 - self.decay)\n","    def copy_to(self, model):\n","        with torch.no_grad():\n","            sd = model.state_dict()\n","            for k in self.shadow:\n","                sd[k].copy_(self.shadow[k])\n","\n","# ---------------------------\n","# Data\n","# ---------------------------\n","\n","def get_cifar10_loader(data_dir, batch_size, num_workers=4):\n","    tfm = transforms.Compose([\n","        transforms.RandomHorizontalFlip(p=0.5),\n","        transforms.ToTensor(),\n","        transforms.Lambda(lambda x: x * 2.0 - 1.0),  # [0,1] -> [-1,1]\n","    ])\n","    train_set = datasets.CIFAR10(root=data_dir, train=True, download=True, transform=tfm)\n","    return DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=True)\n","\n","# ---------------------------\n","# Training loop\n","# ---------------------------\n","\n","def train(args):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    set_seed(args.seed)\n","\n","    # Model\n","    unet = UNet(\n","        in_channels=3,\n","        base_channels=args.base_channels,\n","        channel_mults=(1,2,2,2),\n","        num_res_blocks=2,\n","        time_dim=512,\n","        dropout=0.1,               # 0.1 for CIFAR-10 (paper)\n","        attn_resolutions=(16,),    # attention at 16x16\n","        img_size=32\n","    ).to(device)\n","\n","    diffusion = DDPM(unet, DiffusionConfig(\n","        image_size=32,\n","        channels=3,\n","        timesteps=1000,\n","        beta_start=1e-4,\n","        beta_end=2e-2,\n","        variance_type=args.variance_type\n","    )).to(device)\n","\n","    print(f\"Trainable params: {count_params(diffusion)/1e6:.2f}M\")\n","\n","    # Optimizer\n","    opt = torch.optim.Adam(diffusion.parameters(), lr=2e-4)  # as in paper\n","    scaler = torch.cuda.amp.GradScaler(enabled=args.amp)\n","\n","    # EMA\n","    ema = EMA(diffusion, decay=0.9999)\n","\n","    # Data\n","    loader = get_cifar10_loader(args.data, args.batch_size, args.workers)\n","\n","    os.makedirs(args.out, exist_ok=True)\n","\n","    global_step = 0\n","    diffusion.train()\n","    for epoch in range(args.epochs):\n","        for imgs, _ in loader:\n","            imgs = imgs.to(device)\n","\n","            opt.zero_grad(set_to_none=True)\n","            with torch.cuda.amp.autocast(enabled=args.amp):\n","                loss = diffusion.training_losses(imgs)\n","\n","            scaler.scale(loss).backward()\n","            scaler.step(opt)\n","            scaler.update()\n","\n","            ema.update(diffusion)\n","\n","            if global_step % args.log_interval == 0:\n","                print(f\"[epoch {epoch} step {global_step}] loss: {loss.item():.4f}\")\n","\n","            # Sampling preview with EMA params\n","            if global_step % args.sample_interval == 0 and global_step > 0:\n","                eval_copy = DDPM(UNet(\n","                    in_channels=3,\n","                    base_channels=args.base_channels,\n","                    channel_mults=(1,2,2,2),\n","                    num_res_blocks=2,\n","                    time_dim=512,\n","                    dropout=0.0,\n","                    attn_resolutions=(16,),\n","                    img_size=32\n","                ), diffusion.config).to(device)\n","                eval_copy.load_state_dict(diffusion.state_dict(), strict=True)\n","                ema.copy_to(eval_copy)  # swap in EMA weights\n","\n","                eval_copy.eval()\n","                with torch.no_grad():\n","                    samples = eval_copy.sample(args.nsample, device)\n","                save_image_grid(samples, os.path.join(args.out, f\"samples_step{global_step}.png\"), nrow=int(math.sqrt(args.nsample)))\n","                del eval_copy\n","                torch.cuda.empty_cache()\n","\n","            if global_step % args.ckpt_interval == 0 and global_step > 0:\n","                ckpt = {\n","                    \"model\": diffusion.state_dict(),\n","                    \"opt\": opt.state_dict(),\n","                    \"step\": global_step,\n","                    \"args\": vars(args)\n","                }\n","                torch.save(ckpt, os.path.join(args.out, f\"ddpm_step{global_step}.pt\"))\n","\n","            global_step += 1\n","            if global_step >= args.max_steps:\n","                break\n","        if global_step >= args.max_steps:\n","            break\n","\n","    # final save\n","    ckpt = {\"model\": diffusion.state_dict(), \"opt\": opt.state_dict(), \"step\": global_step, \"args\": vars(args)}\n","    torch.save(ckpt, os.path.join(args.out, f\"ddpm_final.pt\"))\n","\n","def parse_args(argv=None):\n","    p = argparse.ArgumentParser()\n","    p.add_argument(\"--data\", type=str, default=\"./data\")\n","    p.add_argument(\"--out\", type=str, default=\"./runs/ddpm_cifar10\")\n","    p.add_argument(\"--batch_size\", type=int, default=128)\n","    p.add_argument(\"--workers\", type=int, default=4)\n","    p.add_argument(\"--epochs\", type=int, default=1000)\n","    p.add_argument(\"--max_steps\", type=int, default=800_000)\n","    p.add_argument(\"--log_interval\", type=int, default=100)\n","    p.add_argument(\"--sample_interval\", type=int, default=5000)\n","    p.add_argument(\"--ckpt_interval\", type=int, default=50_000)\n","    p.add_argument(\"--nsample\", type=int, default=64)\n","    p.add_argument(\"--base_channels\", type=int, default=128)\n","    p.add_argument(\"--variance_type\", type=str, choices=[\"fixed_small\", \"fixed_large\"], default=\"fixed_small\")\n","    p.add_argument(\"--amp\", action=\"store_true\")\n","    p.add_argument(\"--seed\", type=int, default=42)\n","    return p.parse_known_args(argv)  # <-- ignore unknown args from Jupyter\n","\n","if __name__ == \"__main__\":\n","    args, _ = parse_args()   # args now has .seed and all other attributes\n","    train(args)\n","\n","def parse_args(argv=None):\n","    p = argparse.ArgumentParser()\n","    # ... (same add_argument calls)\n","    return p.parse_known_args(argv)  # <— note: parse_known_args\n","\n","if __name__ == \"__main__\":\n","    args, _unknown = parse_args()    # <— ignore the Jupyter -f arg\n","    train(args)\n","\n"]}]}