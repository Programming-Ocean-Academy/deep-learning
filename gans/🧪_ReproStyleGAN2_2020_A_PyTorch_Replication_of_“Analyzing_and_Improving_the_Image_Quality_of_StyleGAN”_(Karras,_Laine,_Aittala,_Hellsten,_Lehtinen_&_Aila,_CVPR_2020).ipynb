{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìñ Analyzing and Improving the Image Quality of StyleGAN ‚Äì Summary\n",
        "# https://arxiv.org/pdf/1912.04958\n",
        "\n",
        "\n",
        "## üîé Introduction\n",
        "- StyleGAN (2018) achieved state-of-the-art in high-resolution image generation, but showed **characteristic artifacts** (e.g., droplet-like blobs, phase misalignment).\n",
        "- This paper (often called **StyleGAN2**) identifies the causes and **proposes architectural and training improvements**:\n",
        "  - Redesigning normalization inside the generator.\n",
        "  - Removing issues caused by progressive growing.\n",
        "  - Adding a new **path length regularizer** for smoother latent‚Äìimage mapping.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Motivation\n",
        "- **Problems in StyleGAN:**\n",
        "  - Blob-like artifacts from **AdaIN normalization**.\n",
        "  - Progressive growing introduces **phase artifacts** (misaligned features).\n",
        "  - Metrics like FID don‚Äôt fully capture **shape consistency**.\n",
        "- **Goal:** Improve image quality, stability, and **make the generator easier to invert**.\n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ Key Methods\n",
        "\n",
        "### 1. Weight Demodulation\n",
        "- Replaces AdaIN normalization.  \n",
        "- Prevents the generator from sneaking information through spikes in feature maps.  \n",
        "- Removes droplet-like artifacts without losing style control.  \n",
        "\n",
        "### 2. Path Length Regularization\n",
        "- Encourages the generator to map latent vectors to images in a smoother, well-conditioned way.  \n",
        "- Uses Jacobian statistics of the generator to enforce consistent scaling.  \n",
        "- Correlates with **better perceptual quality** (lower Perceptual Path Length ‚Üí smoother latent interpolations).  \n",
        "\n",
        "### 3. Alternative to Progressive Growing\n",
        "- Instead of altering topology during training, StyleGAN2 uses **skip connections** (generator) and **residual networks** (discriminator).  \n",
        "- This avoids phase artifacts while still letting training focus on coarse-to-fine features.  \n",
        "\n",
        "### 4. Larger Networks\n",
        "- Identified a **capacity bottleneck**: StyleGAN underutilized its highest resolutions.  \n",
        "- Doubling channels in high-res layers improved fidelity significantly.  \n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Metrics\n",
        "- **FID (Fr√©chet Inception Distance):** Distribution similarity.  \n",
        "- **Precision & Recall (P&R):** Diversity vs. fidelity.  \n",
        "- **Perceptual Path Length (PPL):** Smoothness and consistency of latent interpolation.  \n",
        "- PPL correlates best with human perception of quality.  \n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Empirical Results\n",
        "- **Artifacts removed:** Blob and phase artifacts eliminated.  \n",
        "- **Better image quality:**  \n",
        "  - StyleGAN2 achieves lower FID and PPL across datasets (FFHQ, LSUN Cats, Cars, Churches, Horses).  \n",
        "  - Improved recall (diversity) while maintaining precision.  \n",
        "- **Projection easier:** StyleGAN2 makes inversion of generated images into latent space more accurate, enabling attribution.  \n",
        "\n",
        "---\n",
        "\n",
        "## üìå Contributions\n",
        "- Introduced **StyleGAN2** with:\n",
        "  - Weight demodulation (artifact-free normalization).  \n",
        "  - Path length regularization (smoother mappings).  \n",
        "  - Redesigned architecture without progressive growing.  \n",
        "- Advanced **state-of-the-art image generation** with better quality and stability.  \n",
        "- Made generated images easier to attribute back to their network of origin.  \n",
        "\n",
        "---\n",
        "\n",
        "## üèÜ Impact\n",
        "- Redefined the **benchmark for unconditional image generation**.  \n",
        "- Widely adopted as the backbone of face, art, and high-fidelity image synthesis.  \n",
        "- Inspired follow-ups: **StyleGAN2-ADA** (adaptive augmentation) and **StyleGAN3** (alias-free generation).  \n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Reference:**  \n",
        "Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., & Aila, T. (2020). *Analyzing and Improving the Image Quality of StyleGAN*. CVPR 2020.\n"
      ],
      "metadata": {
        "id": "xGDcSoHFjMIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîÑ Comparison: StyleGAN (2019) vs. StyleGAN2 (2020)\n",
        "\n",
        "| Aspect                          | StyleGAN (CVPR 2019)                                   | StyleGAN2 (CVPR 2020)                                         |\n",
        "|---------------------------------|--------------------------------------------------------|---------------------------------------------------------------|\n",
        "| **Core Idea**                   | Style-based generator with AdaIN + noise injection     | Redesigned generator with **weight demodulation**             |\n",
        "| **Normalization**               | AdaIN (Adaptive Instance Normalization)                | Removed AdaIN ‚Üí replaced by **weight demod** (artifact-free)  |\n",
        "| **Artifacts**                   | Droplet-like blobs, phase artifacts from progressive growing | Artifacts largely **eliminated** via new design               |\n",
        "| **Latent Space**                | \\( Z \\to W \\) mapping for disentanglement              | Same, but improved smoothness with **path length regularizer** |\n",
        "| **Training Strategy**           | Progressive growing (train from low ‚Üí high res)        | No progressive growing ‚Üí **skip/residual connections** instead |\n",
        "| **Path Length Regularization**  | Not present                                            | Introduced ‚Üí enforces smooth latent‚Äìimage mapping              |\n",
        "| **Capacity**                    | Underutilized high-res layers                          | Doubled channels in high-res layers ‚Üí improved fidelity        |\n",
        "| **Metrics**                     | FID, PPL introduced                                   | FID + **Precision/Recall** for diversity/fidelity trade-off    |\n",
        "| **Datasets**                    | CelebA-HQ, LSUN, FFHQ (introduced)                     | FFHQ, LSUN (Cats, Cars, Churches, Horses)                     |\n",
        "| **Image Quality**               | High, but with artifacts                              | **State-of-the-art**, sharper, artifact-free, more diverse     |\n",
        "| **Impact**                      | Foundation for controllable synthesis (style mixing)   | Redefined benchmark; backbone for StyleGAN2-ADA & StyleGAN3    |\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Key Takeaway:**  \n",
        "StyleGAN introduced **style-based control** (coarse-to-fine disentanglement).  \n",
        "StyleGAN2 fixed **visual artifacts**, improved stability, and became the **new gold standard** for high-fidelity generative modeling.\n"
      ],
      "metadata": {
        "id": "j2Oj1yCFjbL3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp0BbE1ajLJt",
        "outputId": "03058903-901c-4283-88d8-b4d1ca92fecf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7b3218525050>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Imports\n",
        "import math, random, numpy as np\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, utils\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)\n",
        "torch.manual_seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Config (32√ó32 CIFAR-10)\n",
        "class Cfg:\n",
        "    img = 32\n",
        "    z_dim = 128\n",
        "    w_dim = 256\n",
        "    fmap = 128          # base feature maps\n",
        "    batch = 128\n",
        "    epochs = 30\n",
        "    lr = 2e-4\n",
        "    betas = (0.0, 0.99)\n",
        "    r1_gamma = 1.0      # R1 regularization weight (D)\n",
        "    pl_weight = 2.0     # path-length regularization weight (G)\n",
        "    pl_every = 4        # apply PL every N iters\n",
        "    r1_every = 16       # apply R1 every N iters\n",
        "    sample_every = 2\n",
        "\n",
        "cfg = Cfg()\n"
      ],
      "metadata": {
        "id": "AOIl2dyGjwEg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.batch = 32        # try 16 if you still OOM\n",
        "cfg.fmap = 96\n",
        "cfg.w_dim = 192\n",
        "\n",
        "cfg.pl_weight = 1.0   # 0.0 to temporarily disable\n",
        "cfg.pl_every  = 16\n",
        "cfg.r1_gamma  = 0.5\n",
        "cfg.r1_every  = 64\n"
      ],
      "metadata": {
        "id": "p9nWqOw0k0Je"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data: CIFAR-10 in [-1, 1]\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(cfg.img),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5]),\n",
        "])\n",
        "ds = datasets.CIFAR10('./data', train=True, download=True, transform=transform)\n",
        "loader = DataLoader(ds, batch_size=cfg.batch, shuffle=True, drop_last=True, num_workers=2)\n"
      ],
      "metadata": {
        "id": "Y0utoXlDj0Cn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PixelNorm(nn.Module):\n",
        "    def forward(self, x, eps=1e-8):\n",
        "        return x * torch.rsqrt(torch.mean(x**2, dim=1, keepdim=True) + eps)\n",
        "\n",
        "class Mapping(nn.Module):\n",
        "    def __init__(self, z_dim, w_dim, n_layers=4):\n",
        "        super().__init__()\n",
        "        layers = [PixelNorm()]\n",
        "        dim = z_dim\n",
        "        for _ in range(n_layers):\n",
        "            layers += [nn.Linear(dim, w_dim), nn.LeakyReLU(0.2, True)]\n",
        "            dim = w_dim\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self, z):  # [B, z_dim] -> [B, w_dim]\n",
        "        return self.net(z)\n"
      ],
      "metadata": {
        "id": "ozrBG_wrj5UN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModulatedConv2d(nn.Module):\n",
        "    \"\"\"\n",
        "    StyleGAN2 modulated convolution with optional demodulation.\n",
        "    x: [B, C_in, H, W]\n",
        "    w: [B, w_dim] -> style -> scales for C_in\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch, out_ch, w_dim, k=3, up=False, demod=True):\n",
        "        super().__init__()\n",
        "        self.in_ch, self.out_ch, self.k, self.up, self.demod = in_ch, out_ch, k, up, demod\n",
        "        self.weight = nn.Parameter(torch.randn(out_ch, in_ch, k, k) * 0.02)\n",
        "        self.style = nn.Linear(w_dim, in_ch)\n",
        "        self.bias = nn.Parameter(torch.zeros(out_ch))\n",
        "        self.pad = k // 2\n",
        "\n",
        "    def forward(self, x, w):\n",
        "        B, C, H, W = x.shape\n",
        "        if self.up:\n",
        "            x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "            H, W = x.shape[-2:]\n",
        "\n",
        "        s = self.style(w).view(B, 1, self.in_ch, 1, 1)        # [B,1,Cin,1,1]\n",
        "        w_mod = self.weight.unsqueeze(0) * (s + 1.0)          # [B,Cout,Cin,k,k]\n",
        "\n",
        "        if self.demod:\n",
        "            d = torch.rsqrt((w_mod**2).sum(dim=[2,3,4], keepdim=True) + 1e-8)  # [B,Cout,1,1,1]\n",
        "            w_mod = w_mod * d\n",
        "\n",
        "        x = x.view(1, B * self.in_ch, H, W)\n",
        "        w_mod = w_mod.view(B * self.out_ch, self.in_ch, self.k, self.k)\n",
        "        out = F.conv2d(x, w_mod, padding=self.pad, groups=B)\n",
        "        out = out.view(B, self.out_ch, H, W) + self.bias.view(1, -1, 1, 1)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "_PkfSWqBj82_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lrelu(x): return F.leaky_relu(x, 0.2)\n",
        "\n",
        "class SynthesisBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, w_dim, up):\n",
        "        super().__init__()\n",
        "        self.conv1 = ModulatedConv2d(in_ch, out_ch, w_dim, k=3, up=up, demod=True)\n",
        "        self.conv2 = ModulatedConv2d(out_ch, out_ch, w_dim, k=3, up=False, demod=True)\n",
        "        self.toRGB = ModulatedConv2d(out_ch, 3, w_dim, k=1, up=False, demod=False)\n",
        "\n",
        "    def forward(self, x, w1, w2, rgb=None):\n",
        "        x = lrelu(self.conv1(x, w1))\n",
        "        x = lrelu(self.conv2(x, w2))\n",
        "        rgb_new = self.toRGB(x, w2)\n",
        "        rgb = rgb_new if rgb is None else F.interpolate(rgb, scale_factor=2, mode='nearest') + rgb_new\n",
        "        return x, rgb\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    4x4 -> 8 -> 16 -> 32 with skip-to-RGB (StyleGAN2)\n",
        "    \"\"\"\n",
        "    def __init__(self, z_dim, w_dim, fmap):\n",
        "        super().__init__()\n",
        "        self.mapping = Mapping(z_dim, w_dim)\n",
        "        self.const = nn.Parameter(torch.randn(1, fmap*4, 4, 4))\n",
        "        self.b4  = SynthesisBlock(fmap*4, fmap*4, w_dim, up=False) # 4x4\n",
        "        self.b8  = SynthesisBlock(fmap*4, fmap*2, w_dim, up=True)  # 8x8\n",
        "        self.b16 = SynthesisBlock(fmap*2, fmap,   w_dim, up=True)  # 16x16\n",
        "        self.b32 = SynthesisBlock(fmap,   fmap//2,w_dim, up=True)  # 32x32\n",
        "\n",
        "    def forward(self, z, return_w=False):\n",
        "        w = self.mapping(z)                                 # [B, w_dim]\n",
        "        x = self.const.repeat(z.size(0), 1, 1, 1)\n",
        "        rgb = None\n",
        "        # each block consumes two styles (like StyleGAN2)\n",
        "        x, rgb = self.b4 (x, w, w, rgb)\n",
        "        x, rgb = self.b8 (x, w, w, rgb)\n",
        "        x, rgb = self.b16(x, w, w, rgb)\n",
        "        x, rgb = self.b32(x, w, w, rgb)\n",
        "        if return_w: return torch.tanh(rgb), w\n",
        "        return torch.tanh(rgb)\n",
        "\n",
        "class ResBlockD(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, down=True):\n",
        "        super().__init__()\n",
        "        self.down = down\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, 1, 1)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, 1, 1)\n",
        "        self.skip  = nn.Conv2d(in_ch, out_ch, 1, 1, 0)\n",
        "    def forward(self, x):\n",
        "        h = lrelu(self.conv1(x))\n",
        "        h = lrelu(self.conv2(h))\n",
        "        if self.down: h = F.avg_pool2d(h, 2)\n",
        "        s = self.skip(x)\n",
        "        if self.down: s = F.avg_pool2d(s, 2)\n",
        "        return (h + s) / math.sqrt(2)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, fmap):\n",
        "        super().__init__()\n",
        "        c = fmap\n",
        "        self.fromRGB = nn.Conv2d(3, c//2, 1)\n",
        "        self.b32 = ResBlockD(c//2, c,   down=True)\n",
        "        self.b16 = ResBlockD(c,    c*2, down=True)\n",
        "        self.b8  = ResBlockD(c*2,  c*4, down=True)\n",
        "        self.b4  = ResBlockD(c*4,  c*4, down=False)\n",
        "        self.out = nn.Linear(c*4*4*4, 1)\n",
        "    def forward(self, x):\n",
        "        x = lrelu(self.fromRGB(x))\n",
        "        x = self.b32(x); x = self.b16(x); x = self.b8(x); x = self.b4(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.out(x).squeeze(1)\n"
      ],
      "metadata": {
        "id": "7EuYb_gjkAoc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = Generator(cfg.z_dim, cfg.w_dim, cfg.fmap).to(device)\n",
        "D = Discriminator(cfg.fmap).to(device)\n",
        "opt_G = optim.Adam(G.parameters(), lr=cfg.lr, betas=cfg.betas)\n",
        "opt_D = optim.Adam(D.parameters(), lr=cfg.lr, betas=cfg.betas)\n"
      ],
      "metadata": {
        "id": "nDHudGVQkGTL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "g_scaler = GradScaler()\n",
        "d_scaler = GradScaler()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeVn7vjOlAc4",
        "outputId": "6484e460-d937-42e1-c939-f7e673415fb3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-747799327.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  g_scaler = GradScaler()\n",
            "/tmp/ipython-input-747799327.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  d_scaler = GradScaler()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def d_loss_fn(real_logits, fake_logits):\n",
        "    # non-saturating logistic with R1 option outside\n",
        "    return F.softplus(-real_logits).mean() + F.softplus(fake_logits).mean()\n",
        "\n",
        "def g_loss_fn(fake_logits):\n",
        "    return F.softplus(-fake_logits).mean()\n",
        "\n",
        "def r1_regularizer(x, logits):\n",
        "    grad = torch.autograd.grad(outputs=logits.sum(), inputs=x, create_graph=True)[0]\n",
        "    return grad.pow(2).view(grad.size(0), -1).sum(1).mean()\n",
        "\n",
        "def path_length_reg(img, w):\n",
        "    \"\"\"Simplified PL: gradient of (img * noise).sum() w.r.t. w\"\"\"\n",
        "    noise = torch.randn_like(img) / math.sqrt(img.numel() / img.size(0))\n",
        "    grad = torch.autograd.grad(outputs=(img * noise).sum(), inputs=w, create_graph=True)[0]\n",
        "    pl = grad.pow(2).sum(1).sqrt().mean()\n",
        "    return pl\n"
      ],
      "metadata": {
        "id": "mTOC1SOEkJf3"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_z(n): return torch.randn(n, cfg.z_dim, device=device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def show_grid(imgs, title=\"\"):\n",
        "    g = utils.make_grid(imgs, nrow=int(math.sqrt(len(imgs))), normalize=True, value_range=(-1,1))\n",
        "    plt.figure(figsize=(6,6)); plt.imshow(g.permute(1,2,0)); plt.axis('off'); plt.title(title); plt.show()\n"
      ],
      "metadata": {
        "id": "mC5gZ8GIkNBw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G.train(); D.train()\n",
        "g_hist, d_hist = [], []\n",
        "pl_ema = None\n",
        "\n",
        "for epoch in range(cfg.epochs):\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{cfg.epochs}\")\n",
        "    for it, (real, _) in enumerate(pbar):\n",
        "        real = real.to(device)\n",
        "\n",
        "        # ----------------- D step -----------------\n",
        "        z = sample_z(real.size(0))\n",
        "        with torch.no_grad():\n",
        "            fake = G(z)\n",
        "        real_logits = D(real)\n",
        "        fake_logits = D(fake)\n",
        "        d_loss = d_loss_fn(real_logits, fake_logits)\n",
        "\n",
        "        if (it % cfg.r1_every) == 0:\n",
        "            real.requires_grad_(True)\n",
        "            r1 = r1_regularizer(real, D(real))\n",
        "            real.requires_grad_(False)\n",
        "            d_loss = d_loss + (cfg.r1_gamma * 0.5) * r1\n",
        "\n",
        "        opt_D.zero_grad(set_to_none=True)\n",
        "        d_loss.backward()\n",
        "        opt_D.step()\n",
        "\n",
        "        # ----------------- G step -----------------\n",
        "        z = sample_z(real.size(0))\n",
        "        img, w = G(z, return_w=True)\n",
        "        g_loss = g_loss_fn(D(img))\n",
        "\n",
        "        # Path-length regularization every N steps\n",
        "        if (it % cfg.pl_every) == 0:\n",
        "            pl = path_length_reg(img, w)\n",
        "            if pl_ema is None: pl_ema = pl.detach()\n",
        "            pl_pen = (pl - pl_ema).pow(2)\n",
        "            pl_ema = pl_ema.lerp(pl.detach(), 0.01)\n",
        "            g_loss = g_loss + cfg.pl_weight * pl_pen\n",
        "\n",
        "        opt_G.zero_grad(set_to_none=True)\n",
        "        g_loss.backward()\n",
        "        opt_G.step()\n",
        "\n",
        "        g_hist.append(float(g_loss.detach()))\n",
        "        d_hist.append(float(d_loss.detach()))\n",
        "        pbar.set_postfix(g=f\"{g_hist[-1]:.3f}\", d=f\"{d_hist[-1]:.3f}\")\n",
        "\n",
        "    if (epoch+1) % cfg.sample_every == 0:\n",
        "        with torch.no_grad():\n",
        "            imgs = G(sample_z(16)).cpu()\n",
        "        show_grid(imgs, title=f\"Samples ‚Äî epoch {epoch+1}\")\n"
      ],
      "metadata": {
        "id": "tyVs7FIXkP3R"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def style_mix_demo(n_pairs=8):\n",
        "    zA = sample_z(n_pairs); zB = sample_z(n_pairs)\n",
        "    A = G(zA).cpu(); B = G(zB).cpu()\n",
        "\n",
        "    # crude mix: average ws by convex combination to emulate early/late swap (for demo at 32x32)\n",
        "    # (For a fuller impl, expose per-block ws in synthesis and swap after a chosen block.)\n",
        "    mix = G(0.5*zA + 0.5*zB).cpu()\n",
        "\n",
        "    rows = []\n",
        "    for i in range(n_pairs):\n",
        "        rows += [A[i], B[i], mix[i]]\n",
        "    grid = utils.make_grid(torch.stack(rows), nrow=3, normalize=True, value_range=(-1,1))\n",
        "    plt.figure(figsize=(8, 3*n_pairs/3)); plt.imshow(grid.permute(1,2,0)); plt.axis('off')\n",
        "    plt.title(\"Style mixing ‚Äî A / B / A‚ÜíB\"); plt.show()\n",
        "\n",
        "style_mix_demo()\n"
      ],
      "metadata": {
        "id": "t0uGTl8fkUgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G.train(); D.train()\n",
        "g_hist, d_hist = [], []\n",
        "pl_ema = None\n",
        "\n",
        "for epoch in range(cfg.epochs):\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{cfg.epochs}\")\n",
        "    for it, (real, _) in enumerate(pbar):\n",
        "        real = real.to(device, non_blocking=True)\n",
        "\n",
        "        # ----------------- D step (AMP) -----------------\n",
        "        z = torch.randn(real.size(0), cfg.z_dim, device=device)\n",
        "        with torch.no_grad():\n",
        "            with autocast():\n",
        "                fake = G(z)\n",
        "\n",
        "        opt_D.zero_grad(set_to_none=True)\n",
        "        with autocast():\n",
        "            real_logits = D(real)\n",
        "            fake_logits = D(fake)\n",
        "            d_loss = F.softplus(-real_logits).mean() + F.softplus(fake_logits).mean()\n",
        "\n",
        "        # R1 every r1_every iters, computed in fp32 (outside autocast) on a smaller subset\n",
        "        if (it % cfg.r1_every) == 0 and cfg.r1_gamma > 0.0:\n",
        "            mb = min(16, real.size(0))  # microbatch to save memory\n",
        "            real_small = real[:mb].detach().requires_grad_(True)\n",
        "            real_logits_small = D(real_small)  # fp32 since outside autocast\n",
        "            r1 = torch.autograd.grad(outputs=real_logits_small.sum(),\n",
        "                                     inputs=real_small,\n",
        "                                     create_graph=True)[0]\n",
        "            r1 = r1.pow(2).view(mb, -1).sum(1).mean()\n",
        "            d_loss = d_loss + (cfg.r1_gamma * 0.5) * r1\n",
        "\n",
        "        d_scaler.scale(d_loss).backward()\n",
        "        d_scaler.step(opt_D)\n",
        "        d_scaler.update()\n",
        "\n",
        "        # ----------------- G step (AMP) -----------------\n",
        "        z = torch.randn(real.size(0), cfg.z_dim, device=device)\n",
        "\n",
        "        opt_G.zero_grad(set_to_none=True)\n",
        "        with autocast():\n",
        "            img, w = G(z, return_w=True)\n",
        "            g_loss = F.softplus(-D(img)).mean()\n",
        "\n",
        "        # Path-length every pl_every iters, compute in fp32 on microbatch\n",
        "        if (it % cfg.pl_every) == 0 and cfg.pl_weight > 0.0:\n",
        "            mb = min(16, img.size(0))  # micro-PL to reduce graph size\n",
        "            img_small = img[:mb]\n",
        "            w_small = w[:mb]\n",
        "            # compute PL outside autocast to avoid fp16 grad issues\n",
        "            noise = torch.randn_like(img_small) / (img_small.numel() / mb)**0.5\n",
        "            pl = torch.autograd.grad(outputs=(img_small * noise).sum(),\n",
        "                                     inputs=w_small,\n",
        "                                     create_graph=True)[0]\n",
        "            pl = pl.pow(2).sum(1).sqrt().mean()\n",
        "            if pl_ema is None:\n",
        "                pl_ema = pl.detach()\n",
        "            pl_pen = (pl - pl_ema).pow(2)\n",
        "            pl_ema = pl_ema.lerp(pl.detach(), 0.01)\n",
        "            g_loss = g_loss + cfg.pl_weight * pl_pen\n",
        "\n",
        "        g_scaler.scale(g_loss).backward()\n",
        "        g_scaler.step(opt_G)\n",
        "        g_scaler.update()\n",
        "\n",
        "        g_hist.append(float(g_loss.detach()))\n",
        "        d_hist.append(float(d_loss.detach()))\n",
        "        pbar.set_postfix(g=f\"{g_hist[-1]:.3f}\", d=f\"{d_hist[-1]:.3f}\")\n"
      ],
      "metadata": {
        "id": "dv7Qp-xOlmOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use channels-last to save memory bandwidth on Ampere+ GPUs\n",
        "G = G.to(memory_format=torch.channels_last)\n",
        "D = D.to(memory_format=torch.channels_last)\n",
        "\n",
        "# Enable TF32 / higher matmul perf (A100/V100/RTX30+)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n"
      ],
      "metadata": {
        "id": "sWqfjN1ClqOv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}