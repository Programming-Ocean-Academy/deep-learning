{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
        "\n",
        "# https://arxiv.org/pdf/1810.04805\n"
      ],
      "metadata": {
        "id": "ILurvDObSXI9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJzNL8aSR-20",
        "outputId": "d32b208e-33bb-4559-8e96-c3d23825856d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:13<00:00, 12.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 782 | Test batches: 157\n",
            "MiniBERT(\n",
            "  (patch_embed): PatchEmbed(\n",
            "    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
            "  )\n",
            "  (pos_drop): Dropout(p=0.1, inplace=False)\n",
            "  (blocks): ModuleList(\n",
            "    (0-3): 4 x TransformerEncoderBlock(\n",
            "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "      (drop_path1): Dropout(p=0.0, inplace=False)\n",
            "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp): MLP(\n",
            "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
            "        (drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (drop_path2): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "  (head): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n",
            "Trainable params: 0.81 M\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1735213649.py:378: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=cfg.amp)\n",
            "/tmp/ipython-input-1735213649.py:269: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/5 | Train Loss 1.8477 Acc 30.09% | Val Loss 1.6361 Acc 39.69%\n",
            "Epoch 02/5 | Train Loss 1.6253 Acc 39.62% | Val Loss 1.5917 Acc 41.78%\n",
            "Epoch 03/5 | Train Loss 1.4942 Acc 45.15% | Val Loss 1.3975 Acc 48.59%\n",
            "Epoch 04/5 | Train Loss 1.4238 Acc 47.85% | Val Loss 1.3866 Acc 49.57%\n",
            "Epoch 05/5 | Train Loss 1.3747 Acc 49.96% | Val Loss 1.3424 Acc 51.58%\n",
            "Artifacts saved in: /content/mini_bert_outputs\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Mini-BERT: A Didactic Replication of *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding* (2019)\n",
        "==============================================================================================================\n",
        "\n",
        "One-line summary:\n",
        "A compact, classroom-ready Transformer **encoder-only** classifier with a [CLS] token, LayerNorm+Residual blocks,\n",
        "and GELU MLPsâ€”mirroring the *BERT* encoder stackâ€”adapted to **CIFAR-10** by using **patch embeddings** (like ViT)\n",
        "to turn images into token sequences.\n",
        "\n",
        "Teaching notes (mapping BERT â†’ this demo):\n",
        "- BERT core we replicate: multi-layer bidirectional **Transformer encoder** with self-attention, residuals, LayerNorm, and GELU MLP.\n",
        "- [CLS] classification token retained; its final hidden state feeds a linear classifier head.\n",
        "- MLM/NSP pretraining is **omitted** for simplicity; we do **supervised classification** on CIFAR-10.\n",
        "- Images â†’ tokens via **patch embeddings** (Conv2d with kernel=stride=patch_size). This mirrors text-token embeddings in BERT.\n",
        "- Position embeddings are **learnable** (as with BERT).\n",
        "- Name: **MiniBERT** (a didactic stand-in for BERTâ€™s encoder on an image task).\n",
        "\n",
        "Run budget (default):\n",
        "    epochs=5, batch_size=64, lr=1e-3\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms, utils as vutils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# -------------------------\n",
        "# Reproducibility utilities\n",
        "# -------------------------\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False  # allow fast kernels\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# -------------------------\n",
        "# Config (didactic defaults)\n",
        "# -------------------------\n",
        "@dataclass\n",
        "class Config:\n",
        "    # Data / training\n",
        "    data_root: str = \"./data\"\n",
        "    num_classes: int = 10\n",
        "    epochs: int = 5\n",
        "    batch_size: int = 64\n",
        "    lr: float = 1e-3\n",
        "    weight_decay: float = 0.05\n",
        "    num_workers: int = 2\n",
        "\n",
        "    # Image / patches\n",
        "    img_size: int = 32\n",
        "    in_chans: int = 3\n",
        "    patch_size: int = 4  # 32/4 â†’ 8x8 = 64 tokens\n",
        "    drop_rate: float = 0.1\n",
        "\n",
        "    # \"BERT-style\" encoder (mini)\n",
        "    embed_dim: int = 128\n",
        "    depth: int = 4\n",
        "    num_heads: int = 4\n",
        "    mlp_ratio: float = 4.0\n",
        "    attn_drop: float = 0.0\n",
        "    proj_drop: float = 0.0\n",
        "\n",
        "    # Logging / outputs\n",
        "    out_dir: str = \"./mini_bert_outputs\"\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    amp: bool = torch.cuda.is_available()  # mixed precision if CUDA\n",
        "\n",
        "# --------------------------------\n",
        "# Dataset & preprocessing pipeline\n",
        "# --------------------------------\n",
        "def get_cifar10_loaders(cfg: Config) -> Tuple[DataLoader, DataLoader]:\n",
        "    # CIFAR-10 normalization constants\n",
        "    mean = (0.4914, 0.4822, 0.4465)\n",
        "    std = (0.2470, 0.2435, 0.2616)\n",
        "    train_tfms = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(cfg.img_size, padding=4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "    test_tfms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    train_set = datasets.CIFAR10(root=cfg.data_root, train=True, transform=train_tfms, download=True)\n",
        "    test_set  = datasets.CIFAR10(root=cfg.data_root, train=False, transform=test_tfms, download=True)\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=cfg.batch_size, shuffle=True,\n",
        "                              num_workers=cfg.num_workers, pin_memory=True)\n",
        "    test_loader  = DataLoader(test_set, batch_size=cfg.batch_size, shuffle=False,\n",
        "                              num_workers=cfg.num_workers, pin_memory=True)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# --------------------------\n",
        "# Model: Patch Embeddings\n",
        "# --------------------------\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"\n",
        "    Convert image to a sequence of patch embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=128):\n",
        "        super().__init__()\n",
        "        assert img_size % patch_size == 0, \"img_size must be divisible by patch_size\"\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.grid_size = img_size // patch_size\n",
        "        self.num_patches = self.grid_size * self.grid_size\n",
        "\n",
        "        # Conv with kernel=stride=patch_size â†’ non-overlapping patches\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, H, W) â†’ (B, embed_dim, H/ps, W/ps) â†’ (B, N, embed_dim)\n",
        "        x = self.proj(x)  # (B, D, Gh, Gw)\n",
        "        x = x.flatten(2).transpose(1, 2)  # (B, N, D)\n",
        "        return x\n",
        "\n",
        "# --------------------------\n",
        "# Model: BERT-style Encoder\n",
        "# --------------------------\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, out_features, drop=0.0):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = nn.GELU()  # BERT uses GELU\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Mirrors a BERT encoder layer:\n",
        "      - LayerNorm -> Multi-Head Self-Attention -> Residual\n",
        "      - LayerNorm -> MLP (GELU) -> Residual\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim=128, num_heads=4, mlp_ratio=4.0, attn_drop=0.0, proj_drop=0.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=attn_drop, batch_first=True)\n",
        "        self.drop_path1 = nn.Dropout(proj_drop)\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        hidden = int(embed_dim * mlp_ratio)\n",
        "        self.mlp = MLP(embed_dim, hidden, embed_dim, drop=proj_drop)\n",
        "        self.drop_path2 = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Self-attention (bidirectional encoder)\n",
        "        x_res = x\n",
        "        x = self.norm1(x)\n",
        "        x_attn, _ = self.attn(x, x, x, need_weights=False)\n",
        "        x = x_res + self.drop_path1(x_attn)\n",
        "\n",
        "        # MLP\n",
        "        x_res = x\n",
        "        x = self.norm2(x)\n",
        "        x_mlp = self.mlp(x)\n",
        "        x = x_res + self.drop_path2(x_mlp)\n",
        "        return x\n",
        "\n",
        "class MiniBERT(nn.Module):\n",
        "    \"\"\"\n",
        "    MiniBERT Encoder for CIFAR-10:\n",
        "      - Patch embeddings (image â†’ tokens)\n",
        "      - Learnable [CLS] token\n",
        "      - Learnable position embeddings\n",
        "      - Stack of BERT-style encoder blocks\n",
        "      - Classification head on the [CLS] representation\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.patch_embed = PatchEmbed(cfg.img_size, cfg.patch_size, cfg.in_chans, cfg.embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, cfg.embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + num_patches, cfg.embed_dim))\n",
        "        self.pos_drop = nn.Dropout(cfg.drop_rate)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerEncoderBlock(\n",
        "                embed_dim=cfg.embed_dim,\n",
        "                num_heads=cfg.num_heads,\n",
        "                mlp_ratio=cfg.mlp_ratio,\n",
        "                attn_drop=cfg.attn_drop,\n",
        "                proj_drop=cfg.proj_drop\n",
        "            ) for _ in range(cfg.depth)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(cfg.embed_dim)\n",
        "        self.head = nn.Linear(cfg.embed_dim, cfg.num_classes)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # BERT/ViT-style init\n",
        "        nn.init.normal_(self.cls_token, std=0.02)\n",
        "        nn.init.normal_(self.pos_embed, std=0.02)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.LayerNorm):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        x = self.patch_embed(x)  # (B, N, D)\n",
        "\n",
        "        # prepend [CLS]\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, D)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)          # (B, 1+N, D)\n",
        "\n",
        "        # add position embeddings\n",
        "        x = x + self.pos_embed[:, :x.size(1), :]\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        # encoder stack\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # classification on [CLS]\n",
        "        cls_out = x[:, 0]  # (B, D)\n",
        "        logits = self.head(cls_out)\n",
        "        return logits\n",
        "\n",
        "# --------------------------\n",
        "# Training / Evaluation\n",
        "# --------------------------\n",
        "def accuracy(logits, targets):\n",
        "    preds = logits.argmax(dim=1)\n",
        "    return (preds == targets).float().mean().item()\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, scaler, device, epoch, cfg: Config):\n",
        "    model.train()\n",
        "    total_loss, total_acc, total_count = 0.0, 0.0, 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for images, targets in loader:\n",
        "        images, targets = images.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        if cfg.amp:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                logits = model(images)\n",
        "                loss = criterion(logits, targets)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        bs = images.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        total_acc  += accuracy(logits.detach(), targets) * bs\n",
        "        total_count += bs\n",
        "    return total_loss / total_count, total_acc / total_count\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss, total_acc, total_count = 0.0, 0.0, 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for images, targets in loader:\n",
        "        images, targets = images.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, targets)\n",
        "        bs = images.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        total_acc  += accuracy(logits, targets) * bs\n",
        "        total_count += bs\n",
        "    return total_loss / total_count, total_acc / total_count\n",
        "\n",
        "# --------------------------\n",
        "# Utilities: parameter count\n",
        "# --------------------------\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# --------------------------\n",
        "# Visualization\n",
        "# --------------------------\n",
        "def plot_curves(train_losses, val_losses, train_accs, val_accs, out_path):\n",
        "    plt.figure(figsize=(7,5))\n",
        "    plt.plot(train_losses, label=\"Train Loss\")\n",
        "    plt.plot(val_losses, label=\"Val Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"MiniBERT Training/Validation Loss\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(out_path, \"loss_curves.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure(figsize=(7,5))\n",
        "    plt.plot(train_accs, label=\"Train Acc\")\n",
        "    plt.plot(val_accs, label=\"Val Acc\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"MiniBERT Training/Validation Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(out_path, \"accuracy_curves.png\"))\n",
        "    plt.close()\n",
        "\n",
        "@torch.no_grad()\n",
        "def save_prediction_grid(model, loader, class_names, device, out_path, max_images=36):\n",
        "    model.eval()\n",
        "    images, targets = next(iter(loader))\n",
        "    images = images[:max_images].to(device)\n",
        "    logits = model(images)\n",
        "    preds = logits.argmax(dim=1).cpu().tolist()\n",
        "\n",
        "    # De-normalize for visualization\n",
        "    mean = torch.tensor([0.4914, 0.4822, 0.4465], device=images.device).view(1,3,1,1)\n",
        "    std  = torch.tensor([0.2470, 0.2435, 0.2616], device=images.device).view(1,3,1,1)\n",
        "    imgs_vis = (images * std + mean).cpu().clamp(0,1)\n",
        "\n",
        "    grid = vutils.make_grid(imgs_vis, nrow=int(math.sqrt(max_images)))\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.imshow(np.transpose(grid.numpy(), (1,2,0)))\n",
        "    plt.axis(\"off\")\n",
        "    title = \"Predictions: \" + \", \".join(class_names[p] for p in preds[:10]) + \" ...\"\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(out_path, \"predictions_grid.png\"))\n",
        "    plt.close()\n",
        "\n",
        "# --------------------------\n",
        "# Main\n",
        "# --------------------------\n",
        "def main():\n",
        "    set_seed(42)\n",
        "    cfg = Config()\n",
        "    os.makedirs(cfg.out_dir, exist_ok=True)\n",
        "    print(\"==> Using device:\", cfg.device)\n",
        "\n",
        "    # Data\n",
        "    train_loader, test_loader = get_cifar10_loaders(cfg)\n",
        "    class_names = train_loader.dataset.classes\n",
        "    print(f\"Train batches: {len(train_loader)} | Test batches: {len(test_loader)}\")\n",
        "\n",
        "    # Model\n",
        "    model = MiniBERT(cfg).to(cfg.device)\n",
        "    print(model)\n",
        "    print(f\"Trainable params: {count_parameters(model)/1e6:.2f} M\")\n",
        "\n",
        "    # Optimizer & scaler\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=cfg.amp)\n",
        "\n",
        "    # Train / eval\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accs, val_accs = [], []\n",
        "\n",
        "    for epoch in range(1, cfg.epochs + 1):\n",
        "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, scaler, cfg.device, epoch, cfg)\n",
        "        va_loss, va_acc = evaluate(model, test_loader, cfg.device)\n",
        "\n",
        "        train_losses.append(tr_loss); val_losses.append(va_loss)\n",
        "        train_accs.append(tr_acc);    val_accs.append(va_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch:02d}/{cfg.epochs} | \"\n",
        "              f\"Train Loss {tr_loss:.4f} Acc {tr_acc*100:5.2f}% | \"\n",
        "              f\"Val Loss {va_loss:.4f} Acc {va_acc*100:5.2f}%\")\n",
        "\n",
        "    # Save artifacts\n",
        "    torch.save({\"model_state\": model.state_dict(), \"cfg\": cfg.__dict__},\n",
        "               os.path.join(cfg.out_dir, \"mini_bert.ckpt\"))\n",
        "    plot_curves(train_losses, val_losses, train_accs, val_accs, cfg.out_dir)\n",
        "    save_prediction_grid(model, test_loader, class_names, cfg.device, cfg.out_dir, max_images=36)\n",
        "    print(f\"Artifacts saved in: {os.path.abspath(cfg.out_dir)}\")\n",
        "    print(\"Done.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Visualization Block (Loss & Acc)\n",
        "# ================================\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def plot_all_curves(train_losses, val_losses, train_accs, val_accs, out_dir=\"./outputs\"):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # 1. Loss curves\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.plot(train_losses, label=\"Train Loss\", marker=\"o\")\n",
        "    plt.plot(val_losses, label=\"Validation Loss\", marker=\"s\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"MiniBERT Training vs Validation Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(out_dir, \"loss_curve.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # 2. Accuracy curves\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.plot(train_accs, label=\"Train Accuracy\", marker=\"o\")\n",
        "    plt.plot(val_accs, label=\"Validation Accuracy\", marker=\"s\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"MiniBERT Training vs Validation Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(out_dir, \"accuracy_curve.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # 3. Combined figure with 2 subplots\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(14,5))\n",
        "\n",
        "    axs[0].plot(train_losses, label=\"Train Loss\", marker=\"o\")\n",
        "    axs[0].plot(val_losses, label=\"Val Loss\", marker=\"s\")\n",
        "    axs[0].set_title(\"Loss Curves\")\n",
        "    axs[0].set_xlabel(\"Epoch\")\n",
        "    axs[0].set_ylabel(\"Loss\")\n",
        "    axs[0].legend()\n",
        "    axs[0].grid(True, linestyle=\"--\", alpha=0.6)\n",
        "\n",
        "    axs[1].plot(train_accs, label=\"Train Acc\", marker=\"o\")\n",
        "    axs[1].plot(val_accs, label=\"Val Acc\", marker=\"s\")\n",
        "    axs[1].set_title(\"Accuracy Curves\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].set_ylabel(\"Accuracy\")\n",
        "    axs[1].legend()\n",
        "    axs[1].grid(True, linestyle=\"--\", alpha=0.6)\n",
        "\n",
        "    plt.suptitle(\"MiniBERT Training Metrics\", fontsize=14, fontweight=\"bold\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(out_dir, \"metrics_curves.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"ðŸ“Š Metric curves saved in {out_dir}\")\n"
      ],
      "metadata": {
        "id": "RH1CabBvTIKe"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}