{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNRCpN5zGNDkcUsmUo9MvaN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DCtt_s_el1uy","executionInfo":{"status":"ok","timestamp":1755175167114,"user_tz":-480,"elapsed":7143,"user":{"displayName":"Programming Ocean Academy","userId":"12517642345024321372"}},"outputId":"f53fa72a-f363-43dc-bb77-3a2d0b0b976e"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 10, 512])\n"]}],"source":["# Transformer block replication from \"Attention is All You Need\"\n","# Simplified for training affordability (1 block only)\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","\n","class MultiHeadSelfAttention(nn.Module):\n","    def __init__(self, embed_size, heads):\n","        super().__init__()\n","        self.embed_size = embed_size\n","        self.heads = heads\n","        self.head_dim = embed_size // heads\n","\n","        assert self.head_dim * heads == embed_size, \"Embedding size must be divisible by heads\"\n","\n","        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.fc_out = nn.Linear(embed_size, embed_size)\n","\n","    def forward(self, x):\n","        N, seq_len, embed_size = x.shape\n","        x = x.view(N, seq_len, self.heads, self.head_dim)\n","        values = self.values(x)\n","        keys = self.keys(x)\n","        queries = self.queries(x)\n","\n","        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n","        attention = F.softmax(energy / math.sqrt(self.head_dim), dim=-1)\n","        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, seq_len, embed_size)\n","        return self.fc_out(out)\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, embed_size, heads, dropout, forward_expansion):\n","        super().__init__()\n","        self.attention = MultiHeadSelfAttention(embed_size, heads)\n","        self.norm1 = nn.LayerNorm(embed_size)\n","        self.norm2 = nn.LayerNorm(embed_size)\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(embed_size, forward_expansion * embed_size),\n","            nn.ReLU(),\n","            nn.Linear(forward_expansion * embed_size, embed_size)\n","        )\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        attn = self.attention(x)\n","        x = self.dropout(self.norm1(attn + x))\n","        forward = self.feed_forward(x)\n","        return self.dropout(self.norm2(forward + x))\n","\n","# Sample input for testing\n","if __name__ == \"__main__\":\n","    sample_input = torch.rand(32, 10, 512)  # (batch_size, seq_len, embed_size)\n","    block = TransformerBlock(embed_size=512, heads=8, dropout=0.1, forward_expansion=4)\n","    out = block(sample_input)\n","    print(out.shape)  # Expected: (32, 10, 512)\n"]}]}