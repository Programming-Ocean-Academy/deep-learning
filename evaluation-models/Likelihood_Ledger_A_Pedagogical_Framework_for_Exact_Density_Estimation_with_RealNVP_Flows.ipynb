{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"LikelihoodTrainer\")"
      ],
      "metadata": {
        "id": "UgjyM7-aklcr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnZGBq8GkL2F",
        "outputId": "a547a2db-91ec-465c-b874-5429b48476fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing LikelihoodTrainer/liklihood.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile LikelihoodTrainer/liklihood.py\n",
        "\"\"\"\n",
        "Likelihood Ledger — RealNVP Flow + Exact Log-Likelihood (PyTorch)\n",
        "=================================================================\n",
        "An end-to-end teaching project for **likelihood-based evaluation** of generative\n",
        "models using an explicit-density **normalizing flow (RealNVP)**. We train on\n",
        "MNIST (28×28 grayscale → 784-D) and report **exact log-likelihood** and\n",
        "**bits-per-dimension (bpd)** across epochs. Rich visuals and artifacts included.\n",
        "\n",
        "• Dataset: MNIST (default) — optionally Fashion-MNIST\n",
        "• Model: RealNVP with affine coupling, alternating binary masks, random perms\n",
        "• Preprocessing: uniform dequantization + logit transform (with λ-smoothing)\n",
        "• Evaluation: exact log p(x), NLL, bits/dim; validation curves, histograms\n",
        "• Visuals: loss & bpd curves, per-epoch sample grids, NLL histogram, t-SNE of latents\n",
        "• Artifacts: CSV logs, animated GIF timeline of sample grids\n",
        "• Epochs: configurable (5/10/20) for pedagogy\n",
        "\n",
        "Run\n",
        "----\n",
        "python likelihood_flow.py \\\n",
        "  --dataset mnist \\\n",
        "  --data_root ./data \\\n",
        "  --outdir ./outputs_ll \\\n",
        "  --epochs 10 \\\n",
        "  --batch_size 128 \\\n",
        "  --layers 8 \\\n",
        "  --hidden 512 \\\n",
        "  --lambda_logit 0.05\n",
        "\n",
        "Optional flags: --dataset {mnist,fashion}, --epochs {5,10,20}, --lr 1e-3, --device cuda\n",
        "\n",
        "Notes\n",
        "-----\n",
        "• **Exact likelihood** comes from change of variables: log p(x) = log p(z) + log|det ∂z/∂x|.\n",
        "• RealNVP’s triangular Jacobian makes log-determinant **tractable**.\n",
        "• The **logit** preprocessing stabilizes flow training on image pixels in [0,1].\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import math\n",
        "import argparse\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, utils as vutils\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "\n",
        "try:\n",
        "    from sklearn.manifold import TSNE\n",
        "    _HAVE_SK = True\n",
        "except Exception:\n",
        "    _HAVE_SK = False\n",
        "\n",
        "# ------------------------------\n",
        "# Reproducibility\n",
        "# ------------------------------\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# ------------------------------\n",
        "# Data & Transforms (Dequantize + Logit)\n",
        "# ------------------------------\n",
        "\n",
        "class LogitTransform(nn.Module):\n",
        "    \"\"\"Applies x ∈ [0,1]  →  x' = λ + (1-2λ)x  →  y = logit(x'), with\n",
        "    log|det J| = Σ[ log(1-2λ) - log(x') - log(1-x') ].\"\"\"\n",
        "    def __init__(self, lam: float = 0.05, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.lam = lam\n",
        "        self.eps = eps\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # x in [0,1]\n",
        "        x_ = self.lam + (1 - 2 * self.lam) * x\n",
        "        x_ = x_.clamp(self.eps, 1 - self.eps)\n",
        "        y = torch.log(x_) - torch.log(1 - x_)\n",
        "        logdet = torch.sum(\n",
        "            torch.log(torch.tensor(1 - 2 * self.lam, device=x.device))\n",
        "            - torch.log(x_) - torch.log(1 - x_), dim=list(range(1, x.dim()))\n",
        "        )\n",
        "        return y, logdet\n",
        "    def inverse(self, y: torch.Tensor) -> torch.Tensor:\n",
        "        x_ = torch.sigmoid(y)\n",
        "        x = (x_ - self.lam) / (1 - 2 * self.lam)\n",
        "        return x.clamp(0.0, 1.0)\n",
        "\n",
        "\n",
        "def get_dataloaders(dataset: str, data_root: str, batch_size: int, workers: int):\n",
        "    tf = transforms.ToTensor()  # yields [0,1]\n",
        "    if dataset == 'mnist':\n",
        "        train_set = datasets.MNIST(root=data_root, train=True, download=True, transform=tf)\n",
        "        test_set  = datasets.MNIST(root=data_root, train=False, download=True, transform=tf)\n",
        "        channels = 1; H = W = 28\n",
        "    elif dataset == 'fashion':\n",
        "        train_set = datasets.FashionMNIST(root=data_root, train=True, download=True, transform=tf)\n",
        "        test_set  = datasets.FashionMNIST(root=data_root, train=False, download=True, transform=tf)\n",
        "        channels = 1; H = W = 28\n",
        "    else:\n",
        "        raise ValueError('dataset must be mnist or fashion')\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=workers, drop_last=True)\n",
        "    test_loader  = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=workers)\n",
        "    return train_loader, test_loader, channels, H, W\n",
        "\n",
        "# ------------------------------\n",
        "# RealNVP Building Blocks\n",
        "# ------------------------------\n",
        "\n",
        "class Permute(nn.Module):\n",
        "    \"\"\"Fixed random permutation layer for 1D vectors.\"\"\"\n",
        "    def __init__(self, D: int, seed: int = 0):\n",
        "        super().__init__()\n",
        "        rng = np.random.default_rng(seed)\n",
        "        perm = torch.tensor(rng.permutation(D), dtype=torch.long)\n",
        "        inv = torch.empty_like(perm)\n",
        "        inv[perm] = torch.arange(D)\n",
        "        self.register_buffer('perm', perm)\n",
        "        self.register_buffer('inv', inv)\n",
        "    def forward(self, x):\n",
        "        return x[:, self.perm]\n",
        "    def inverse(self, y):\n",
        "        return y[:, self.inv]\n",
        "\n",
        "class CouplingMLP(nn.Module):\n",
        "    def __init__(self, D_in: int, D_out: int, hidden: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(D_in, hidden), nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
        "            nn.Linear(hidden, D_out)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class AffineCoupling(nn.Module):\n",
        "    \"\"\"RealNVP affine coupling layer with a binary mask m ∈ {0,1}^D.\n",
        "    y_A = x_A\n",
        "    y_B = x_B * exp(s(x_A)) + t(x_A)\n",
        "    log|det J| = Σ s(x_A) over B dims.\n",
        "    \"\"\"\n",
        "    def __init__(self, mask: torch.Tensor, hidden: int):\n",
        "        super().__init__()\n",
        "        self.register_buffer('m', mask.float())\n",
        "        D = mask.numel()\n",
        "        D_A = int(self.m.sum().item())\n",
        "        D_B = D - D_A\n",
        "        self.s_net = CouplingMLP(D_A, D_B, hidden)\n",
        "        self.t_net = CouplingMLP(D_A, D_B, hidden)\n",
        "        self.scale = nn.Parameter(torch.zeros(1))  # global scale for stability\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        x_A = x * self.m                       # keep A\n",
        "        x_A_flat = x_A[:, self.m.bool()]        # (N, D_A)\n",
        "        s = self.s_net(x_A_flat)\n",
        "        t = self.t_net(x_A_flat)\n",
        "        s = torch.tanh(s) * torch.exp(self.scale)  # stabilize\n",
        "        # apply to B\n",
        "        x_B = x[:, (~self.m.bool())]\n",
        "        y_B = x_B * torch.exp(s) + t\n",
        "        y = torch.empty_like(x)\n",
        "        y[:, self.m.bool()] = x_A_flat\n",
        "        y[:, (~self.m.bool())] = y_B\n",
        "        logdet = torch.sum(s, dim=1)\n",
        "        return y, logdet\n",
        "    def inverse(self, y: torch.Tensor) -> torch.Tensor:\n",
        "        y_A = y[:, self.m.bool()]\n",
        "        s = self.s_net(y_A)\n",
        "        t = self.t_net(y_A)\n",
        "        s = torch.tanh(s) * torch.exp(self.scale)\n",
        "        y_B = y[:, (~self.m.bool())]\n",
        "        x_B = (y_B - t) * torch.exp(-s)\n",
        "        x = torch.empty_like(y)\n",
        "        x[:, self.m.bool()] = y_A\n",
        "        x[:, (~self.m.bool())] = x_B\n",
        "        return x\n",
        "\n",
        "class RealNVP(nn.Module):\n",
        "    def __init__(self, D: int, layers: int = 8, hidden: int = 512, seed: int = 0):\n",
        "        super().__init__()\n",
        "        ms = []\n",
        "        perms = []\n",
        "        rng = np.random.default_rng(seed)\n",
        "        # alternating 0/1 masks with random perms in between\n",
        "        base_mask = torch.zeros(D)\n",
        "        base_mask[::2] = 1.0  # start with even dims as A\n",
        "        for k in range(layers):\n",
        "            # permute dims to increase mixing\n",
        "            perm = Permute(D, seed=int(rng.integers(0, 10_000)))\n",
        "            perms.append(perm)\n",
        "            # flip mask pattern each layer\n",
        "            mask = base_mask.clone() if (k % 2 == 0) else (1 - base_mask)\n",
        "            ms.append(AffineCoupling(mask, hidden))\n",
        "        self.perms = nn.ModuleList(perms)\n",
        "        self.couplings = nn.ModuleList(ms)\n",
        "        self.D = D\n",
        "    def f(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # x -> z, accumulate logdet\n",
        "        logdet_total = torch.zeros(x.size(0), device=x.device)\n",
        "        h = x\n",
        "        for perm, coup in zip(self.perms, self.couplings):\n",
        "            h = perm(h)\n",
        "            h, logdet = coup(h)\n",
        "            logdet_total = logdet_total + logdet\n",
        "        return h, logdet_total\n",
        "    def inv(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        h = z\n",
        "        for perm, coup in reversed(list(zip(self.perms, self.couplings))):\n",
        "            h = coup.inverse(h)\n",
        "            h = perm.inverse(h)\n",
        "        return h\n",
        "\n",
        "# ------------------------------\n",
        "# Utilities: sampling, logging, plots\n",
        "# ------------------------------\n",
        "\n",
        "def save_image_grid(tensor: torch.Tensor, path: str, nrow: int = 8):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    grid = vutils.make_grid(tensor, nrow=nrow, padding=2)\n",
        "    vutils.save_image(grid, path)\n",
        "\n",
        "def save_csv(path: str, header: List[str], rows: List[List]):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with open(path, 'w') as f:\n",
        "        f.write(','.join(header) + '\\n')\n",
        "        for r in rows:\n",
        "            f.write(','.join(map(str, r)) + '\\n')\n",
        "\n",
        "def make_gif_from_folder(folder: str, out_path: str, fps: int = 4):\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    frames = []\n",
        "    if not os.path.isdir(folder):\n",
        "        return\n",
        "    for name in sorted(os.listdir(folder)):\n",
        "        if name.lower().endswith('.png'):\n",
        "            frames.append(imageio.v2.imread(os.path.join(folder, name)))\n",
        "    if frames:\n",
        "        imageio.mimsave(out_path, frames, fps=fps)\n",
        "\n",
        "def plot_curves(curves: dict, path: str, title: str, ylabel: str):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    for k, v in curves.items():\n",
        "        xs = list(range(1, len(v)+1))\n",
        "        plt.plot(xs, v, label=k)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n",
        "\n",
        "# ------------------------------\n",
        "# Training & Evaluation\n",
        "# ------------------------------\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    dataset: str = 'mnist'            # mnist or fashion\n",
        "    data_root: str = './data'\n",
        "    outdir: str = './outputs_ll'\n",
        "    batch_size: int = 128\n",
        "    workers: int = 2\n",
        "    epochs: int = 10\n",
        "    lr: float = 1e-3\n",
        "    layers: int = 8\n",
        "    hidden: int = 512\n",
        "    lambda_logit: float = 0.05\n",
        "    seed: int = 42\n",
        "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class LikelihoodTrainer:\n",
        "    def __init__(self, cfg: TrainConfig):\n",
        "        self.cfg = cfg\n",
        "        self.device = torch.device(cfg.device)\n",
        "        set_seed(cfg.seed)\n",
        "        os.makedirs(cfg.outdir, exist_ok=True)\n",
        "\n",
        "        # Data\n",
        "        self.train_loader, self.test_loader, C, H, W = get_dataloaders(cfg.dataset, cfg.data_root, cfg.batch_size, cfg.workers)\n",
        "        self.D = C * H * W\n",
        "        self.logit = LogitTransform(cfg.lambda_logit)\n",
        "\n",
        "        # Model\n",
        "        self.flow = RealNVP(D=self.D, layers=cfg.layers, hidden=cfg.hidden, seed=cfg.seed).to(self.device)\n",
        "        self.opt = optim.Adam(self.flow.parameters(), lr=cfg.lr)\n",
        "\n",
        "        # Logs\n",
        "        self.hist_train_nll: List[float] = []\n",
        "        self.hist_val_nll: List[float] = []\n",
        "        self.hist_val_bpd: List[float] = []\n",
        "\n",
        "        # Fixed noise for generation from base\n",
        "        self.fixed_z = torch.randn(64, self.D, device=self.device)\n",
        "        self.C, self.H, self.W = C, H, W\n",
        "\n",
        "    def _preprocess_batch(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Dequantize, logit, flatten. Returns (y, logdet_logit).\"\"\"\n",
        "        # x in [0,1]; uniform dequantization\n",
        "        u = torch.rand_like(x)\n",
        "        x_deq = (x * 255.0 + u) / 256.0\n",
        "        y, logdet = self.logit(x_deq)\n",
        "        y = y.view(y.size(0), -1)  # flatten to (N,D)\n",
        "        return y, logdet\n",
        "\n",
        "    def _nll_batch(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Compute per-sample negative log-likelihood and bits/dim.\"\"\"\n",
        "        y, logdet_logit = self._preprocess_batch(x)\n",
        "        z, logdet_flow = self.flow.f(y)\n",
        "        # Standard normal log-prob\n",
        "        log_pz = -0.5 * (z**2 + math.log(2 * math.pi)).sum(dim=1)\n",
        "        log_px = log_pz + logdet_flow + logdet_logit\n",
        "        nll = -log_px  # per-sample\n",
        "        bpd = nll / (self.D * math.log(2))\n",
        "        return nll, bpd\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self):\n",
        "        self.flow.eval()\n",
        "        nlls, bpds = [], []\n",
        "        for x, _ in self.test_loader:\n",
        "            x = x.to(self.device)\n",
        "            nll, bpd = self._nll_batch(x)\n",
        "            nlls.append(nll.cpu()); bpds.append(bpd.cpu())\n",
        "        nlls = torch.cat(nlls)\n",
        "        bpds = torch.cat(bpds)\n",
        "        return float(nlls.mean().item()), float(bpds.mean().item()), nlls, bpds\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, n: int = 64) -> torch.Tensor:\n",
        "        self.flow.eval()\n",
        "        z = torch.randn(n, self.D, device=self.device)\n",
        "        y = self.flow.inv(z)\n",
        "        x = self.logit.inverse(y.view(n, self.C, self.H, self.W))\n",
        "        return x\n",
        "\n",
        "    def train_epoch(self, epoch: int):\n",
        "        self.flow.train()\n",
        "        running = 0.0\n",
        "        for i, (x, _) in enumerate(self.train_loader):\n",
        "            x = x.to(self.device)\n",
        "            nll, _ = self._nll_batch(x)\n",
        "            loss = nll.mean()\n",
        "            self.opt.zero_grad(); loss.backward(); self.opt.step()\n",
        "            running += loss.item()\n",
        "            if (i+1) % 100 == 0:\n",
        "                print(f\"Epoch {epoch:03d} [{i+1:04d}/{len(self.train_loader)}]  NLL: {loss.item():.2f}\")\n",
        "        self.hist_train_nll.append(running / len(self.train_loader))\n",
        "\n",
        "        # Save sample grid for the epoch\n",
        "        with torch.no_grad():\n",
        "            sample = self.sample(64)\n",
        "            save_image_grid(sample, os.path.join(self.cfg.outdir, f\"samples/epoch_{epoch:03d}.png\"), nrow=8)\n",
        "\n",
        "    def post_plots(self):\n",
        "        # Curves\n",
        "        plot_curves({\"Train NLL\": self.hist_train_nll, \"Val NLL\": self.hist_val_nll},\n",
        "                    os.path.join(self.cfg.outdir, \"curves/nll_curve.png\"),\n",
        "                    title=\"Negative Log-Likelihood over Epochs\", ylabel=\"NLL (lower is better)\")\n",
        "        plot_curves({\"Val bpd\": self.hist_val_bpd},\n",
        "                    os.path.join(self.cfg.outdir, \"curves/bpd_curve.png\"),\n",
        "                    title=\"Bits per Dimension over Epochs\", ylabel=\"bits/dim (lower is better)\")\n",
        "        # Animated GIF of samples\n",
        "        make_gif_from_folder(os.path.join(self.cfg.outdir, \"samples\"),\n",
        "                             os.path.join(self.cfg.outdir, \"samples/timeline.gif\"), fps=3)\n",
        "\n",
        "    def run(self):\n",
        "        for epoch in range(1, self.cfg.epochs + 1):\n",
        "            self.train_epoch(epoch)\n",
        "            val_nll, val_bpd, nlls, bpds = self.evaluate()\n",
        "            self.hist_val_nll.append(val_nll); self.hist_val_bpd.append(val_bpd)\n",
        "            print(f\"[VAL] Epoch {epoch:03d}  NLL = {val_nll:.2f}  |  bpd = {val_bpd:.4f}\")\n",
        "\n",
        "            # NLL histogram\n",
        "            os.makedirs(os.path.join(self.cfg.outdir, 'curves'), exist_ok=True)\n",
        "            plt.figure(figsize=(6,4))\n",
        "            plt.hist(nlls.numpy(), bins=40, alpha=0.8)\n",
        "            plt.title(f\"Per-sample NLL (epoch {epoch})\")\n",
        "            plt.xlabel(\"NLL\"); plt.ylabel(\"Count\"); plt.tight_layout()\n",
        "            plt.savefig(os.path.join(self.cfg.outdir, f\"curves/nll_hist_epoch_{epoch:03d}.png\")); plt.close()\n",
        "\n",
        "            # (Optional) t-SNE of latents for a single batch\n",
        "            if _HAVE_SK:\n",
        "                x_vis, _ = next(iter(self.test_loader))\n",
        "                x_vis = x_vis.to(self.device)\n",
        "                y_vis, _ = self._preprocess_batch(x_vis)\n",
        "                z_vis, _ = self.flow.f(y_vis)\n",
        "                emb = TSNE(n_components=2, init='random', random_state=42, perplexity=30).fit_transform(z_vis.detach().cpu().numpy())\n",
        "                plt.figure(figsize=(5,5))\n",
        "                plt.scatter(emb[:,0], emb[:,1], s=6, alpha=0.6)\n",
        "                plt.title(f\"t-SNE of Latent z (epoch {epoch})\")\n",
        "                plt.tight_layout(); plt.savefig(os.path.join(self.cfg.outdir, f\"curves/tsne_latent_{epoch:03d}.png\")); plt.close()\n",
        "\n",
        "        # Save logs\n",
        "        save_csv(os.path.join(self.cfg.outdir, 'logs/nll.csv'), ['epoch','train_nll','val_nll'],\n",
        "                 [[i+1, self.hist_train_nll[i], self.hist_val_nll[i]] for i in range(len(self.hist_val_nll))])\n",
        "        save_csv(os.path.join(self.cfg.outdir, 'logs/bpd.csv'), ['epoch','val_bpd'],\n",
        "                 [[i+1, self.hist_val_bpd[i]] for i in range(len(self.hist_val_bpd))])\n",
        "\n",
        "        self.post_plots()\n",
        "\n",
        "        print(\"\\nArtifacts saved:\")\n",
        "        print(\" - Sample grids:\", os.path.join(self.cfg.outdir, \"samples/epoch_###.png\"))\n",
        "        print(\" - Sample animation:\", os.path.join(self.cfg.outdir, \"samples/timeline.gif\"))\n",
        "        print(\" - Curves:\", os.path.join(self.cfg.outdir, \"curves/\"))\n",
        "        print(\" - Logs (CSV):\", os.path.join(self.cfg.outdir, \"logs/\"))\n",
        "        print(\"Training complete. Outputs saved to:\", self.cfg.outdir)\n",
        "\n",
        "# ------------------------------\n",
        "# Main\n",
        "# ------------------------------\n",
        "\n",
        "def parse_args() -> 'TrainConfig':\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument('--dataset', type=str, default='mnist')\n",
        "    p.add_argument('--data_root', type=str, default='./data')\n",
        "    p.add_argument('--outdir', type=str, default='./outputs_ll')\n",
        "    p.add_argument('--batch_size', type=int, default=128)\n",
        "    p.add_argument('--workers', type=int, default=2)\n",
        "    p.add_argument('--epochs', type=int, default=10)\n",
        "    p.add_argument('--lr', type=float, default=1e-3)\n",
        "    p.add_argument('--layers', type=int, default=8)\n",
        "    p.add_argument('--hidden', type=int, default=512)\n",
        "    p.add_argument('--lambda_logit', type=float, default=0.05)\n",
        "    p.add_argument('--seed', type=int, default=42)\n",
        "    p.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    args = p.parse_args(args=None if __name__ == '__main__' else [])\n",
        "    return TrainConfig(**vars(args))\n",
        "\n",
        "\n",
        "def main():\n",
        "    cfg = parse_args()\n",
        "    print(\"\\nLikelihood Ledger — RealNVP Flow + Exact Log-Likelihood (PyTorch)\")\n",
        "    print(\"Config:\", cfg)\n",
        "    trainer = LikelihoodTrainer(cfg)\n",
        "    trainer.run()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python LikelihoodTrainer/liklihood.py \\\n",
        "  --dataset mnist \\\n",
        "  --epochs 10 \\\n",
        "  --layers 8 \\\n",
        "  --hidden 512 \\\n",
        "  --lambda_logit 0.05 \\\n",
        "  --outdir ./outputs_ll\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwhKwQiIkVRO",
        "outputId": "499f867b-6bf2-4796-ed3a-47d75610f7ed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Likelihood Ledger — RealNVP Flow + Exact Log-Likelihood (PyTorch)\n",
            "Config: TrainConfig(dataset='mnist', data_root='./data', outdir='./outputs_ll', batch_size=128, workers=2, epochs=10, lr=0.001, layers=8, hidden=512, lambda_logit=0.05, seed=42, device='cuda')\n",
            "100% 9.91M/9.91M [00:01<00:00, 5.01MB/s]\n",
            "100% 28.9k/28.9k [00:00<00:00, 132kB/s]\n",
            "100% 1.65M/1.65M [00:01<00:00, 1.27MB/s]\n",
            "100% 4.54k/4.54k [00:00<00:00, 16.1MB/s]\n",
            "Epoch 001 [0100/468]  NLL: -1462.94\n",
            "Epoch 001 [0200/468]  NLL: -2013.84\n",
            "Epoch 001 [0300/468]  NLL: -1608.11\n",
            "Epoch 001 [0400/468]  NLL: -2325.74\n",
            "[VAL] Epoch 001  NLL = -2435.83  |  bpd = -4.4823\n",
            "Epoch 002 [0100/468]  NLL: -2428.37\n",
            "Epoch 002 [0200/468]  NLL: -2571.56\n",
            "Epoch 002 [0300/468]  NLL: -2573.00\n",
            "Epoch 002 [0400/468]  NLL: -2098.82\n",
            "[VAL] Epoch 002  NLL = -2482.48  |  bpd = -4.5682\n",
            "Epoch 003 [0100/468]  NLL: -2641.11\n",
            "Epoch 003 [0200/468]  NLL: -2665.64\n",
            "Epoch 003 [0300/468]  NLL: -2712.14\n",
            "Epoch 003 [0400/468]  NLL: -2682.07\n",
            "[VAL] Epoch 003  NLL = -2705.81  |  bpd = -4.9791\n",
            "Epoch 004 [0100/468]  NLL: -776.56\n",
            "Epoch 004 [0200/468]  NLL: -1840.71\n",
            "Epoch 004 [0300/468]  NLL: -1998.58\n",
            "Epoch 004 [0400/468]  NLL: -1953.73\n",
            "[VAL] Epoch 004  NLL = -2153.89  |  bpd = -3.9635\n",
            "Epoch 005 [0100/468]  NLL: -2297.49\n",
            "Epoch 005 [0200/468]  NLL: -2400.69\n",
            "Epoch 005 [0300/468]  NLL: -2361.11\n",
            "Epoch 005 [0400/468]  NLL: -2341.82\n",
            "[VAL] Epoch 005  NLL = -2371.05  |  bpd = -4.3631\n",
            "Epoch 006 [0100/468]  NLL: -2427.50\n",
            "Epoch 006 [0200/468]  NLL: -2442.50\n",
            "Epoch 006 [0300/468]  NLL: -2436.69\n",
            "Epoch 006 [0400/468]  NLL: -2455.87\n",
            "[VAL] Epoch 006  NLL = -2285.77  |  bpd = -4.2062\n",
            "Epoch 007 [0100/468]  NLL: -2504.00\n",
            "Epoch 007 [0200/468]  NLL: -2591.95\n",
            "Epoch 007 [0300/468]  NLL: -2511.62\n",
            "Epoch 007 [0400/468]  NLL: -2565.02\n",
            "[VAL] Epoch 007  NLL = -2537.83  |  bpd = -4.6700\n",
            "Epoch 008 [0100/468]  NLL: -2577.35\n",
            "Epoch 008 [0200/468]  NLL: -2631.60\n",
            "Epoch 008 [0300/468]  NLL: -2604.30\n",
            "Epoch 008 [0400/468]  NLL: -2607.42\n",
            "[VAL] Epoch 008  NLL = -2532.02  |  bpd = -4.6594\n",
            "Epoch 009 [0100/468]  NLL: -2561.59\n",
            "Epoch 009 [0200/468]  NLL: -2591.89\n",
            "Epoch 009 [0300/468]  NLL: -2619.21\n",
            "Epoch 009 [0400/468]  NLL: -2680.33\n",
            "[VAL] Epoch 009  NLL = -2600.56  |  bpd = -4.7855\n",
            "Epoch 010 [0100/468]  NLL: -2646.26\n",
            "Epoch 010 [0200/468]  NLL: -2586.77\n",
            "Epoch 010 [0300/468]  NLL: -2617.82\n",
            "Epoch 010 [0400/468]  NLL: -2623.75\n",
            "[VAL] Epoch 010  NLL = -2598.80  |  bpd = -4.7822\n",
            "\n",
            "Artifacts saved:\n",
            " - Sample grids: ./outputs_ll/samples/epoch_###.png\n",
            " - Sample animation: ./outputs_ll/samples/timeline.gif\n",
            " - Curves: ./outputs_ll/curves/\n",
            " - Logs (CSV): ./outputs_ll/logs/\n",
            "Training complete. Outputs saved to: ./outputs_ll\n"
          ]
        }
      ]
    }
  ]
}